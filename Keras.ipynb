{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitakshra/Hello-world/blob/master/Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRpF2sGcfHj-"
      },
      "source": [
        "#A Crash Course on Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pttHeKcVL1vF"
      },
      "source": [
        "What is tensorflow\n",
        "\n",
        "Tensor Flow is a multidimesional array. It is represented as tf.Tensor object\n",
        "Tensor flow use Keras as an API for building frame work which is written in python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GiStfrKruwZ"
      },
      "source": [
        "TensorFlow has two main components Graphs and Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNJM-IJ4riaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee33c20-60b8-4f09-a155-2a5468c175b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow\",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjkIIJEvsON5"
      },
      "outputs": [],
      "source": [
        "# Calulating the rank of a tensor\n",
        "var1 = tf.Variable(\"hello to tensor programming\",tf.string) # a scalar tensor\n",
        "var2 = tf.Variable(127,tf.int32)# a scalar tensor\n",
        "var3 = tf.Variable(53.62,tf.float32)# a scalar tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdtHSCaWs9Di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c33ed2b-7273-4435-ec07-eef4005c642a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(tf.rank(var1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFp2fNoOtdpr"
      },
      "outputs": [],
      "source": [
        "print(tf.rank(var2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXl_qKrvterM"
      },
      "outputs": [],
      "source": [
        "print(tf.rank(var3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHMToxtxtopL"
      },
      "outputs": [],
      "source": [
        "var11 = tf.Variable(['hello','to','tensor','programming'],tf.string)\n",
        "var22 = tf.Variable([127,123,45],tf.int32)\n",
        "var33 = tf.Variable([53.62,12.36,3.65],tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwshPjJmt_U2"
      },
      "outputs": [],
      "source": [
        "print(tf.rank(var11))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMk5UKQpuEoG"
      },
      "outputs": [],
      "source": [
        "print(tf.rank(var22))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0nJEQtbuH9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dae9b57-f671-4637-d42d-4ec49452a88b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(tf.rank(var33))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JdNfC1QuO4B"
      },
      "outputs": [],
      "source": [
        "# creating 2D tensor\n",
        "var_2 = tf.Variable([['hello','to'],['tensor','programming'],['good','work']],tf.string)\n",
        "var_3 = tf.Variable([[127,123],[12,34],[34,45]],tf.int32)\n",
        "var_4 = tf.Variable([[53.62,12.36],[23.54,33.22],[12.22,3.65]],tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_fEb0viu2NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2351f47d-1f2b-4750-defc-4e9686a1caaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(tf.rank(var_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTDT1CU7u4yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5193a9-11b2-436f-d83d-4f27c2c38ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(tf.rank(var_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS7oyrH_u-mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae0ece4-2ac6-4add-b19f-3713afd4bb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(tf.rank(var_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7lICrpHvCVu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3170e92-8c65-4f55-b33a-8ff60d13e26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(25, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "#components of a tensor\n",
        "# Variables\n",
        "# Nodes\n",
        "# Edges\n",
        "\n",
        "# Addition of two values using tensors\n",
        "import tensorflow as tf\n",
        "x = tf.constant(15)\n",
        "y = tf.constant(10)\n",
        "z = tf.add(x,y)\n",
        "## sess = tf.Session()\n",
        "#out_z = sess.run(z)\n",
        "#print(out_z)\n",
        "print(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WFBG0ct9Y0g"
      },
      "outputs": [],
      "source": [
        "# note the some examples which i am using here are taken from tensorflow.org\n",
        "# rank in tensor flow\n",
        "rank_0 = tf.constant(8) # scalar tensor\n",
        "print(rank_0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on-3su6JRx53"
      },
      "outputs": [],
      "source": [
        "# rank1 tensor\n",
        "rank_1  =tf.constant([2.3,3.6,5.2])\n",
        "print(rank_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_MzTgsqSGGB"
      },
      "outputs": [],
      "source": [
        "# rank2 tensor\n",
        "rank_2 = tf.constant([[1,2,3],[2,4,5],[4,3,6]])\n",
        "print(rank_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq8mj2FiSYCQ"
      },
      "outputs": [],
      "source": [
        "# rank3 tensor\n",
        "rank_3 = tf.constant([\n",
        "                      [\n",
        "                       [1,2],[2,3],[3,4]\n",
        "                      ],\n",
        "                      [\n",
        "                       [7,7],[9,7],[5,6]\n",
        "                      ]\n",
        "])\n",
        "print(rank_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eVDOXLSUCkx"
      },
      "outputs": [],
      "source": [
        "# some basic operations on tensors\n",
        "a = tf.constant([[1,2],[2,3]])\n",
        "b = tf.constant([[5,6],[8,9]])\n",
        "c = tf.multiply(a,b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmmJV6ZHkums"
      },
      "outputs": [],
      "source": [
        "# indexing\n",
        "# indexing start at 0\n",
        "# negative indices count backwards from the end\n",
        "# colons, :, are used to have slices start : stop : step\n",
        "rank1_1_tensor = tf.constant([0,1,2,3,4,5,6,10,45,78,96,58])\n",
        "print(rank1_1_tensor.ndim)\n",
        "print(rank1_1_tensor.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_VkiY8xllL2"
      },
      "outputs": [],
      "source": [
        "print(\"First:\", rank1_1_tensor[0].numpy())\n",
        "print(\"Second:\", rank1_1_tensor[1].numpy())\n",
        "print(\"Last:\", rank1_1_tensor[-1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG8f2TRTlk-t"
      },
      "outputs": [],
      "source": [
        "print(\"Everything:\", rank1_1_tensor[:].numpy())\n",
        "print(\"Before 6:\", rank1_1_tensor[:6].numpy())\n",
        "print(\"From 3 to the end:\", rank1_1_tensor[3:].numpy())\n",
        "print(\"From 3, before 7:\", rank1_1_tensor[2:7].numpy())\n",
        "print(\"Every other item:\", rank1_1_tensor[::2].numpy())\n",
        "print(\"Reversed:\", rank1_1_tensor[::-1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDI0EWixmtEl"
      },
      "source": [
        "Operation of two dimensional axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyCkvqgLmsTP"
      },
      "outputs": [],
      "source": [
        "print(rank_2.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTIncpSEmCd5"
      },
      "outputs": [],
      "source": [
        "# Pull out a single value from a 2-rank tensor\n",
        "print(rank_2[1, 1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFHYnQ4FoqMz"
      },
      "outputs": [],
      "source": [
        "# Get row and column tensors\n",
        "print(\"Second row:\", rank_2[1, :].numpy())\n",
        "print(\"Second column:\", rank_2[:, 1].numpy())\n",
        "print(\"Last row:\", rank_2[-1, :].numpy())\n",
        "print(\"First item in last column:\", rank_2[0, -1].numpy())\n",
        "print(\"Skip the first row:\")\n",
        "print(rank_2[1:, :].numpy(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgMHvgr7ox9l"
      },
      "outputs": [],
      "source": [
        "print(rank_3[:, :, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0FJcMyno_Y9"
      },
      "outputs": [],
      "source": [
        "# Shape returns a `TensorShape` object that shows the size along each axis\n",
        "x = tf.constant([[1], [2], [3]])\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjsk4c82pOww"
      },
      "source": [
        "# Introduction to Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbaf9QX2pHKF"
      },
      "outputs": [],
      "source": [
        "# A tensorflow variable is a recomended way to shared  persistent state of a program\n",
        "# tf.Variables is used to store those tensors which you want to modify during operations\n",
        "# keras which is used to build neural network use variables to store variables\n",
        "import tensorflow as tf\n",
        "# directly creating a variable \n",
        "var1 = tf.Variable(12)\n",
        "# indirectly creating a variable using constant \n",
        "var2 = tf.constant(12.5)\n",
        "var2_conv = tf.Variable(var2)\n",
        "\n",
        "my_tensor = tf.constant([[1.2,2.2],[2.3,3.6]])\n",
        "my_variable = tf.Variable(my_tensor)\n",
        "\n",
        "# variables can be Boolean Variables or complex numbers\n",
        "\n",
        "print(\"Shape: \", var2_conv.shape)\n",
        "print(\"DType : \", var2_conv.dtype)\n",
        "print(\"As numpy array\", var2_conv.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_NPnNxUDcTh"
      },
      "outputs": [],
      "source": [
        "print(\"A variable\",my_variable)\n",
        "print(\"\\n Index of highest value\",tf.argmax(my_variable))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW69TcM5FZVY"
      },
      "outputs": [],
      "source": [
        "# operations on tensor using variables\n",
        "a = tf.Variable([2.0,3.0])\n",
        "# assign value of a on b\n",
        "b = tf.Variable(a)\n",
        "print(\"a is \",a)\n",
        "print(\"b is \",b)\n",
        "# assign new values to a\n",
        "a.assign([3.5,4.6])\n",
        "\n",
        "print(a) # new values assign to a\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4BPCSf0HPKC"
      },
      "outputs": [],
      "source": [
        "# some other important versions of assign\n",
        "print(a.assign_add([1.1,2.3]).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n7dO3dEHexM"
      },
      "outputs": [],
      "source": [
        "print(b.assign_sub([1.1,2.3]).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tAFe9LXHjxb"
      },
      "outputs": [],
      "source": [
        "# two variables having the same name but will have different values i.e. backed by \n",
        "# different tensors\n",
        "import tensorflow as tf\n",
        "tensor1 = tf.constant([[1,2],[3,4]])\n",
        "a = tf.Variable(tensor1, name='Mark')\n",
        "b = tf.Variable(tensor1+1, name='Mark')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcD_obi_Kryz"
      },
      "outputs": [],
      "source": [
        "print(a==b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9VyuJqRK0FT"
      },
      "outputs": [],
      "source": [
        "# variables are used for differntiation we we see next but there are some variables\n",
        "# which we dont want to get differntiated we set trainable = False\n",
        "step_counter = tf.Variable(1,trainable=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvl4gVT9LU9w"
      },
      "outputs": [],
      "source": [
        "# multiplication of two tensors\n",
        "a = tf.Variable([[1.0,2.0,3.5],[3.6,2.5,2.7]])\n",
        "b = tf.constant([[1.0,2.0],[1.5,2.3],[2.2,3.2]])\n",
        "c = tf.matmul(a,b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKyFZ3gyNnMZ"
      },
      "source": [
        "# Automatic Differentiation and Gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShLQ_BagMhjj"
      },
      "outputs": [],
      "source": [
        "# Automatic Differentiation is useful for implememting ML algo using BackProp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(5.6)\n",
        "\n",
        "# create a tape to hold the computed value in it\n",
        "with tf.GradientTape(x) as tape:\n",
        "  y = x*x+2*x\n",
        "\n",
        "dy_dx = tape.gradient(y,x)\n",
        "# since a tensor is beong returen so we compare it to numpy()\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk2IPNKRMhTc"
      },
      "outputs": [],
      "source": [
        "# building gradient tape for models\n",
        "w = tf.Variable(tf.random.normal((4,2)),name='W')\n",
        "b = tf.Variable(tf.zeros(2,dtype=tf.float32),name='b')\n",
        "x = [[1.0,2.0,3.0,4.0]]\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x@w + b\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "[dl_w,dl_b] = tape.gradient(loss,[w,b])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFoiJaiwF74s"
      },
      "outputs": [],
      "source": [
        "dl_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cliq8Ki7GGla"
      },
      "outputs": [],
      "source": [
        "# represnting the difference between trainable and non trainable variables\n",
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0,name='X0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(4.0,name = 'X1',trainable=False)\n",
        "# operation on a trainable value \n",
        "# when we add a contant to a variable then it become a tensor\n",
        "x2 = tf.Variable(2.0, name = 'X2')+3.0\n",
        "# Constant not a variable so can't take the gradient\n",
        "x3 = tf.constant(3.0, name='X3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**3 + x1**2 + x2*3 + x3)\n",
        "grad = tape.gradient(y,[x0,x1,x2,x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTeLqly-wXCw"
      },
      "source": [
        "Eager Execution Vs Graph Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2UEJ3pFwWYt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "from datetime import datetime\n",
        "\n",
        "# Define a Python Function\n",
        "def py_func(x,y,b):\n",
        "  x = tf.matmul(x,y) # a matrix multiplication\n",
        "  x = x+b # adding b\n",
        "  return x\n",
        "# a_function_that_uses_a_graph is a Tensor FLow function\n",
        "func_graph = tf.function(py_func)\n",
        "# make some tensors\n",
        "x1 = tf.constant([[1.0,2.0]])\n",
        "y1 = tf.constant([[2.0],[3.0]])\n",
        "b1 = tf.constant(2.5)\n",
        "\n",
        "# value computed from original func.\n",
        "org_val = py_func(x1,y1,b1)\n",
        "\n",
        "# we use a graph converted function\n",
        "tf_func_val = func_graph(x1,y1,b1)\n",
        "\n",
        "# assert the values\n",
        "assert(org_val==tf_func_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scW582zedM5v"
      },
      "outputs": [],
      "source": [
        "# from outside a Function looks like a regular function we write using tensoflow \n",
        "# but in underground in encapsulate several graphs created using tf.Graph API\n",
        "# to make a simple function Function we use decorator @tf.function\n",
        "def inner_function(x,y,b):\n",
        "  x = tf.matmul(x,y)\n",
        "  x = x+b\n",
        "  return x\n",
        "# use decorator to make outer function Function\n",
        "@tf.function\n",
        "def outer_function(x):\n",
        "  y = tf.constant([[2.0],[3.0]])\n",
        "  b = tf.constant(4.0)\n",
        "  return inner_function(x,y,b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zodkefMCahwZ"
      },
      "outputs": [],
      "source": [
        "# a graph will going to be created b the callable\n",
        "val = outer_function(tf.constant([[2.3,26]])).numpy()\n",
        "print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPteyVh9b7M4"
      },
      "outputs": [],
      "source": [
        "# polymorphism implemenattion using @tf.function \n",
        "# exmaple taken form tensoflow.org\n",
        "# take graph execution based function relu_func using decorator\n",
        "@tf.function\n",
        "def relu_func(x):\n",
        "  return tf.maximum(0.,x)\n",
        "print(relu_func(tf.constant(2.5)))\n",
        "print(relu_func(tf.constant([2.2,2.6])))\n",
        "print(relu_func(tf.Variable([2.1,-.36])))\n",
        "print(relu_func(tf.Variable([[2.1,-.36]])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsCSTc2o1zrT"
      },
      "source": [
        "one more example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkVvWk1c12hS"
      },
      "outputs": [],
      "source": [
        "def innerproduct(x):\n",
        "  y = tf.matmul(x,x)\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toR6ZqpY2Bob"
      },
      "outputs": [],
      "source": [
        "print(\"Hello{0} \".format(innerproduct([[5]])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0CWEjWK2P9_"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def innerproduct1(x):\n",
        "  y = tf.matmul(x,x)\n",
        "  return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjeFMgh32dKA"
      },
      "outputs": [],
      "source": [
        "print(\"{}\".format(innerproduct1([[5]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW3a2kQ9r_Ic"
      },
      "source": [
        "#Flexible Dense Module from scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rnhdG-OXdBO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqGu7LCJ2811"
      },
      "source": [
        "tf.Module(\n",
        "    name=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9q0xVunsF45"
      },
      "outputs": [],
      "source": [
        "class FlexibleDenseModule(tf.Module):\n",
        "  def __init__(self, out_features, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.is_built = False\n",
        "    self.out_features = out_features\n",
        "  def __call__(self,x):\n",
        "    if not self.is_built:\n",
        "      self.w = tf.Variable(tf.random.normal([x.shape[-1], self.out_features]),name='w')\n",
        "      self.b = tf.Variable(tf.zeros([self.out_features]), name='b')\n",
        "      self.is_built = True\n",
        "    y = tf.matmul(x,self.w) + self.b\n",
        "    return tf.nn.relu(y)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf4D4kZOW-qR"
      },
      "outputs": [],
      "source": [
        "# create a module by calling flexible layer\n",
        "class FlexiSequenceModel(tf.Module):\n",
        "  def __init__(self,name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self.Dense_1 = FlexibleDenseModule(out_features=3)\n",
        "    self.Dense_2 = FlexibleDenseModule(out_features=2)\n",
        "  def __call__(self, x):\n",
        "    x = self.Dense_1(x) # making a call to callable of FlexibleDenseModule\n",
        "    return self.Dense_2(x) # fitting the o/p of first hiden layer to nxt lyr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM1fdY_Qywag"
      },
      "outputs": [],
      "source": [
        "flexi_model = FlexiSequenceModel(name = \"New_Flexi_model\") # naming the model and \n",
        "# creating the reference of FlexiSequenceModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuyHoYk7zU-2"
      },
      "outputs": [],
      "source": [
        "# calling the callable\n",
        "fll(texi_modef.constant([[4.6,6.3,2.2,9.3]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlRxv5Sg2eQ1"
      },
      "outputs": [],
      "source": [
        "# A module can have many layers as we have seen\n",
        "# saving the weights of a module or sub module is called checkpointing\n",
        "# or we can say called \"saving a module\"\n",
        "# steps to create a check point are\n",
        "# step 1\n",
        "# name of the check point\n",
        "chkp_point = \"my_checkpoint\"\n",
        "checkpoint = tf.train.Checkpoint(model = flexi_model)\n",
        "checkpoint.write(chkp_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKaheRNZ2eIl"
      },
      "outputs": [],
      "source": [
        "# to check what check points have\n",
        "# just have to use\n",
        "tf.train.list_variables(chkp_point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3_1x02n5sOe"
      },
      "outputs": [],
      "source": [
        "# loading back model\n",
        "new_model = FlexiSequenceModel()\n",
        "new_chkp_point = tf.train.Checkpoint(model=new_model)\n",
        "new_chkp_point.restore(\"my_checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMOMm6PV6ovw"
      },
      "outputs": [],
      "source": [
        "# should be the same\n",
        "new_model(tf.constant([[2.0, 2.0, 2.0,3.6]]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESGFhP00fxF1"
      },
      "source": [
        "# Building a dense layer from scratch in tensorflow\n",
        "\n",
        "It is imporant that we can control number of neurons in each layer\n",
        "\n",
        "by controlling number no. of cols in trainable parameter matrix i.e W\n",
        "to implement this we use two classes \n",
        "class1 Dense\n",
        "class 2 Sequential Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXEMX2OU5mdZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2f5UiF3fwuv"
      },
      "outputs": [],
      "source": [
        "class Dense(tf.Module):\n",
        "  def __init__(self, in_features, out_features, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.w = tf.Variable(tf.random.normal([in_features,out_features]),name='w')\n",
        "    self.b = tf.Variable(tf.zeros([out_features]), name='b')\n",
        "    print(self.w)\n",
        "  def __call__(self,x):\n",
        "    y = tf.matmul(x,self.w) + self.b\n",
        "    return tf.nn.relu(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtQmrS-ze7V6"
      },
      "outputs": [],
      "source": [
        "class Sequential_mdl(tf.Module):\n",
        "  def __init__(self,name = None):\n",
        "    super().__init__(name=name)\n",
        "    # creating layers\n",
        "\n",
        "    self.dense_1 = Dense(in_features = 3,out_features=3)\n",
        "    self.dense_2 = Dense(in_features = 3,out_features=2)\n",
        "  def __call__(self, x):\n",
        "    x = self.dense_1(x)\n",
        "    return self.dense_2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBzROTJRzMF3"
      },
      "outputs": [],
      "source": [
        "my_first_model = Sequential_mdl(\"My_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnYskLZKz1mH"
      },
      "outputs": [],
      "source": [
        "print(my_first_model(tf.constant([[5.2,7.2,5.3]])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxnZOvRk9gB6"
      },
      "source": [
        "# Introduction to Tensor Slicing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Zx5bYG9ggt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Ah-3bT9hTa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.seed(10)\n",
        "print(random.random())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzv-gLT19gjr"
      },
      "outputs": [],
      "source": [
        "t1 = tf.constant([8,9,2,4,5,6,25,32,41,12])\n",
        "# code for tensor slicing\n",
        "print(tf.slice(t1, begin = [1], size=[3])\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88ExHi-c9hW6"
      },
      "outputs": [],
      "source": [
        "# normal slicing\n",
        "t1[2:6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1nROPxm9hVK"
      },
      "outputs": [],
      "source": [
        "# create  a two dimension tensor\n",
        "import random\n",
        "t2_vl = []\n",
        "for i in range(4):\n",
        "  t2 = []\n",
        "  for j in range(5):\n",
        "    t2.append(random.randint(1,100))\n",
        "  t2_vl.append(t2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5TXnzAr9hNf"
      },
      "outputs": [],
      "source": [
        "t2_vl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK8UQcXJCE4C"
      },
      "outputs": [],
      "source": [
        "tf_t2 = tf.constant(t2_vl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL-m-dpLCMAk"
      },
      "outputs": [],
      "source": [
        "tf_t2[:-1,1:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd1f3-8rCp7M"
      },
      "outputs": [],
      "source": [
        "t3 = tf.constant([[[1, 3, 5, 7],\n",
        "                   [9, 11, 13, 15]],\n",
        "                  [[17, 19, 21, 23],\n",
        "                   [25, 27, 29, 31]]\n",
        "                  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lReJ2RJMDf4k"
      },
      "outputs": [],
      "source": [
        "print(tf.slice(t3,\n",
        "               begin=[1, 1, 1],\n",
        "               size=[1, 1, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViDyUNgpDrVe"
      },
      "source": [
        "Ragged Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWWaz0v8EAgs"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUNiHQVTEDgq"
      },
      "outputs": [],
      "source": [
        "# to declare the tensor as ragged we have to declare as tf.ragged\n",
        "numeric = tf.ragged.constant([[3,1,4,2],[1,2,4],[6,8,4,3,8],[],[3,5]])\n",
        "words = tf.ragged.constant([[\"wow\",\"thats\",\"nice\"],['to','have','python'],[],['for','data','science']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqVIIX8aEDZc"
      },
      "outputs": [],
      "source": [
        "print(tf.add(numeric,4)) # adding 4 to numeric ragged tensor using add\n",
        "print(tf.reduce_mean(numeric,axis=1)) # take mean coloumn wise\n",
        "print(tf.concat([numeric,tf.constant([[1,2,3]])],axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cggrXHlgEDVa"
      },
      "outputs": [],
      "source": [
        "print(tf.tile(numeric,(1,2))) # new mtx rows: 1*numeric_rows, 2*numeric_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUzBLAESEDMd"
      },
      "outputs": [],
      "source": [
        "print(tf.tile(numeric,(2,1))) # new mtx rows: 2*numeric_rows, 1*numeric_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDUjmqKrNCNu"
      },
      "outputs": [],
      "source": [
        "print(tf.strings.substr(words, 0, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf50-UToNNIH"
      },
      "outputs": [],
      "source": [
        "print(tf.map_fn(tf.math.square, numeric))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnz66-cfNaSb"
      },
      "source": [
        "# Sparse Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui6ZtfdDNNE8"
      },
      "outputs": [],
      "source": [
        "# sparse tensor is used to build a time efficent tensor ehich can reduce a Dense \n",
        "# tensor to a minimum space tensor\n",
        "# tensorFlow uses tf.SparseTensor object\n",
        "import tensorflow as tf\n",
        "\n",
        "spt1 = tf.SparseTensor(indices=[[0,1],[2,4],[1,3]],values=[21,34,22],dense_shape=[4,10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R8_H2yCNNAb"
      },
      "outputs": [],
      "source": [
        "print(spt1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsckOAkEQP1-"
      },
      "outputs": [],
      "source": [
        "st2 = tf.sparse.from_dense([[1, 0, 0, 8,0], [1, 0, 0, 0,0], [1, 0, 3, 0,0],[0,0,0,9,0]])\n",
        "print(print(st2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDfTGPfVw89Q"
      },
      "source": [
        "Addition of two tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqj80hC0QPti"
      },
      "outputs": [],
      "source": [
        "sp_a = tf.SparseTensor(indices=[[0, 2], [3, 4]],\n",
        "                       values=[31, 2], \n",
        "                       dense_shape=[4, 10])\n",
        "\n",
        "sp_b = tf.SparseTensor(indices=[[0, 2], [7, 0]],\n",
        "                       values=[56, 38],\n",
        "                       dense_shape=[4, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLHWxheTxS-C"
      },
      "outputs": [],
      "source": [
        "sp_sum = tf.sparse.add(sp_a, sp_b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04hk2w0_yhsL"
      },
      "outputs": [],
      "source": [
        "print(sp_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4vdqKbSdewm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVU0SzY0fM4_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential,models\n",
        "from tensorflow.keras.layers import Dense,Flatten\n",
        "from tensorflow.keras import layers\n",
        "print(\"Tensorflow\",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOVsEvUEhNPh"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([[10,20,35],[42,54,68]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgw8Xu0zOnyO"
      },
      "outputs": [],
      "source": [
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJLRZYAbO1UQ"
      },
      "outputs": [],
      "source": [
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUwhNb8HO1Ra"
      },
      "outputs": [],
      "source": [
        "print(x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya2vdNrdO1Np"
      },
      "outputs": [],
      "source": [
        "# tf.Tensor has two attributes Tensor.shape and Tensor.dtype for size and type of elts in tensor repectively\n",
        "x+x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wQ0mH7-O1MM"
      },
      "outputs": [],
      "source": [
        "# multiply two tensors\n",
        "x@tf.transpose(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KsEFW2OQVZP"
      },
      "outputs": [],
      "source": [
        "# concatenating tensors\n",
        "tf.concat([x,x,x],axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cObGDOaVQoaG"
      },
      "source": [
        "variables in tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmhNqjKoQVWY"
      },
      "outputs": [],
      "source": [
        "var = tf.Variable([0.0, 0.0, 0.0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLEiTLS9QVSx"
      },
      "outputs": [],
      "source": [
        "var.assign([1, 2, 3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPlloidEQVRN"
      },
      "outputs": [],
      "source": [
        "var.assign_add([1, 1, 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTitXSvLQVNl"
      },
      "outputs": [],
      "source": [
        "# Create some operation in tensors\n",
        "a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
        "c = tf.matmul(a, b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZfXnEAAQVLs"
      },
      "outputs": [],
      "source": [
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx_L5_edTHl4"
      },
      "source": [
        "How to make a single layer single neuron model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AwsmwJkTGtJ"
      },
      "outputs": [],
      "source": [
        "# input_shape is the shape of the tensor\n",
        "# units = number of neurons\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(units=1,input_shape=[1])                           \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7udX0SZQVH3"
      },
      "outputs": [],
      "source": [
        "# when we make a model it is important to compile it\n",
        "# compilation has two important steps optimizer and loss\n",
        "#lr = 0.1\n",
        "model.compile(loss = 'mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er8eq1hDU6DU"
      },
      "outputs": [],
      "source": [
        "# y = 3*x + 5.0\n",
        "def eqn(x):\n",
        "  return 3.0*x+5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khDkqg0ybQnf"
      },
      "outputs": [],
      "source": [
        "from random import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvAkS8cAaxDv"
      },
      "outputs": [],
      "source": [
        "x = [random() for i in range(50)]\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cRiCBaWQVF_"
      },
      "outputs": [],
      "source": [
        "y = [eqn(val) for val in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODQu7roubnBP"
      },
      "outputs": [],
      "source": [
        "train_x = x[0:35]\n",
        "train_y = y[0:35]\n",
        "test_x = x[35:]\n",
        "test_y = y[35:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji3iQM83b55K"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_x,train_y,epochs = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqZ60vyacXyj"
      },
      "outputs": [],
      "source": [
        "# plot the loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(history.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3HkUuWAcXvK"
      },
      "outputs": [],
      "source": [
        "pred_y = model.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx_V6sTmdIgO"
      },
      "outputs": [],
      "source": [
        "print(mean_squared_error(test_y,pred_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "619FM4DxepfA"
      },
      "source": [
        "Concept of Dense Layers\n",
        "\n",
        "Dense layer means fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCtTPjHk1M0y"
      },
      "outputs": [],
      "source": [
        "l0 = tf.keras.layers.Dense(units=4,input_shape = [1])\n",
        "l1 = tf.keras.layers.Dense(units=4)\n",
        "l2 = tf.keras.layers.Dense(units=1)\n",
        "model_dense = Sequential([l0,l1,l2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7wxeSLX1MxF"
      },
      "outputs": [],
      "source": [
        "model_dense.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzbQCyHk1Mt8"
      },
      "outputs": [],
      "source": [
        "history_dense = model_dense.fit(train_x,train_y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6Kgdvec1Mqd"
      },
      "outputs": [],
      "source": [
        "print(history.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fe8iOj91Mm0"
      },
      "outputs": [],
      "source": [
        "# plot the loss and accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(history_dense.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s57Ws7kE260d"
      },
      "outputs": [],
      "source": [
        "pred_y_dense = model_dense.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Td5XjVL1Mg2"
      },
      "outputs": [],
      "source": [
        "print(mean_squared_error(test_y,pred_y_dense))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmjcTGw41Mdl"
      },
      "outputs": [],
      "source": [
        "# have a look at the weight variables\n",
        "print(\"These are the weight variables of l0 layers : {}\".format(l0.get_weights()))\n",
        "print(\"These are the weight variables of l1 layers : {}\".format(l1.get_weights()))\n",
        "print(\"These are the weight variables of l2 layers : {}\".format(l2.get_weights()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBh9J5O7M3l8"
      },
      "source": [
        "Linear Regression using Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZTUSLK11MaT"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp87rxOh1MW0"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D7ttOLH1MTa"
      },
      "outputs": [],
      "source": [
        "downloaded3 = drive.CreateFile({'id':'1cMVDgjI3Wmks9_oRO3O6Ama9UeZqw7a1'}) # replace the id with id of file you want to access\n",
        "downloaded3.GetContentFile('car_details.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp9rB5_AlZLq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "from pandas import DataFrame,Series\n",
        "from sklearn.preprocessing import StandardScaler #for Normalization\n",
        "from sklearn.linear_model import LinearRegression # fitting a Linear Regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "pd.set_option('display.max_columns',None)\n",
        "#setting the dimesnsions of the plot\n",
        "sns.set(rc = {'figure.figsize':(12,9)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMSWo04ilBEb"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Activation\n",
        "model = Sequential()\n",
        "model.add(Dense(5,activation='relu',input_dim = 3))\n",
        "model.add(Dense(6))\n",
        "model.add(Dense(1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAan5yLLPBuX"
      },
      "source": [
        "Information about data\n",
        "\n",
        "year: Year of the car when it was bought\n",
        "\n",
        "selling price: Price at which the car is being sold\n",
        "\n",
        "present price: This is the current ex-showroom price of the car.\n",
        "\n",
        "km driven: Number of Kilometres the car is driven\n",
        "\n",
        "fuel: car is driven text_format fuel sort Fuel type of car (petrol / diesel / CNG / LPG / electric)\n",
        "\n",
        "seller type: Tells if a Seller is Individual or a Dealer\n",
        "\n",
        "transmission: Gear transmission of the car (Automatic/Manual)\n",
        "\n",
        "owner: Number of previous owners of the car."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Gchvi-Oys-"
      },
      "outputs": [],
      "source": [
        "# Feature Enginering\n",
        "df_car_price = pd.read_csv('car_details.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZW5SUqjMPLzm"
      },
      "outputs": [],
      "source": [
        "df_car_price.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtrdCJ27PWtr"
      },
      "outputs": [],
      "source": [
        "df_car_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X0rHu3wQC4T"
      },
      "outputs": [],
      "source": [
        "mileage = df_car_price['mileage']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4vOgOAQRHpp"
      },
      "outputs": [],
      "source": [
        "mileage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YR7I5BgRHlh"
      },
      "outputs": [],
      "source": [
        "milg_int = []\n",
        "val = 0\n",
        "for mlf in mileage:\n",
        "  s = \"0\"\n",
        "  for val in str(mlf):\n",
        "    if val not in ['0','1','2','3','4','5','6','7','8','9','.']:\n",
        "      break\n",
        "    else:\n",
        "      s = s+val\n",
        "  milg_int.append((s.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INik5vzJRHiq"
      },
      "outputs": [],
      "source": [
        "milg_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG1fKRAGRHeB"
      },
      "outputs": [],
      "source": [
        "mileg_new = list(map(float,milg_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaxnG_ADRHZL"
      },
      "outputs": [],
      "source": [
        "print(mileg_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY-pMiQxRdQj"
      },
      "outputs": [],
      "source": [
        "engine = df_car_price['engine']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pmiEC8IRdyL"
      },
      "outputs": [],
      "source": [
        "engine_int = []\n",
        "val = 0\n",
        "for eng in engine:\n",
        "  s = \"0\"\n",
        "  for val in str(eng):\n",
        "    if val not in ['0','1','2','3','4','5','6','7','8','9','.']:\n",
        "      break\n",
        "    else:\n",
        "      s = s+val\n",
        "  engine_int.append((s.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umfHCyJ6Rd18"
      },
      "outputs": [],
      "source": [
        "engine_new= list(map(int,engine_int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDuL9c3oZocv"
      },
      "outputs": [],
      "source": [
        "engine_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "979uR5_5Rd5m"
      },
      "outputs": [],
      "source": [
        "max_power = df_car_price['max_power']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uajW4LV_ReXs"
      },
      "outputs": [],
      "source": [
        "max_power_flt = []\n",
        "val = 0\n",
        "for mxp in max_power:\n",
        "  s = \"0\"\n",
        "  for val in str(mxp):\n",
        "    if val not in ['0','1','2','3','4','5','6','7','8','9','.']:\n",
        "      break\n",
        "    else:\n",
        "      s = s+val\n",
        "  max_power_flt.append((s.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKCvcqTRRuQE"
      },
      "outputs": [],
      "source": [
        "max_power_new= list(map(float,engine_int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYlO_ZHZR0gf"
      },
      "source": [
        "**Now** we have to take some new features and create a new data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yDBr834R1N8"
      },
      "outputs": [],
      "source": [
        "car_selling_pred = DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q66J7TpR4Ym"
      },
      "outputs": [],
      "source": [
        "car_selling_pred['max_power'] = max_power_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KXzlUFVR4Ts"
      },
      "outputs": [],
      "source": [
        "car_selling_pred['engine_power'] = engine_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fg4dpClR95w"
      },
      "outputs": [],
      "source": [
        "car_selling_pred['mileage'] = mileg_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uD2ccOESAGg"
      },
      "outputs": [],
      "source": [
        "car_selling_pred['Selling_price'] = df_car_price['selling_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTPRW-UeZzQ_"
      },
      "outputs": [],
      "source": [
        "car_selling_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9roGXxHSBaL"
      },
      "outputs": [],
      "source": [
        "s_scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we-wLWzzSBWo"
      },
      "outputs": [],
      "source": [
        "car_selling_norm = s_scaler.fit_transform(car_selling_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYaIsgRMSAC3"
      },
      "outputs": [],
      "source": [
        "df_car_selling_norm = DataFrame(data=car_selling_norm,columns=['max_power','engine_power','mileage','selling_price'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH6mYmUjaDzx"
      },
      "outputs": [],
      "source": [
        "df_car_selling_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k0iDykLSYiH"
      },
      "outputs": [],
      "source": [
        "xx = df_car_selling_norm[['max_power','engine_power','mileage']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48AF34ZVSkAW"
      },
      "outputs": [],
      "source": [
        "y_norm = df_car_selling_norm['selling_price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAJCpYwPSzGH"
      },
      "outputs": [],
      "source": [
        "xx1 = xx.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPLHgdIcSy8n"
      },
      "outputs": [],
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(xx1,y_norm,test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWwZF4HUltQ7"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CITT0AgjpN7p"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_x,train_y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bbesCwvqQE8"
      },
      "outputs": [],
      "source": [
        "# plot the loss for linear regression model of car price prediction\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(history.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzK16tBEad9w"
      },
      "outputs": [],
      "source": [
        "y_pred_car = model.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeyUliFkalKo"
      },
      "outputs": [],
      "source": [
        "print(mean_squared_error(test_y,y_pred_car))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOsWH3z7Pm7H"
      },
      "source": [
        "Functional Layer in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2wJ3KLjegzd"
      },
      "source": [
        "Some important points :\n",
        "Keras Functional API is used to build more flexibile and robust model than in comparison to tf.keras.Sequential API\n",
        "The main idea is that a deep learning model is usually a directed acyclic graph (DAG) of layers. So the functional API is a way to build graphs of layers.(TensorFlow.org)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_RmMagJhWfi"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh-JWiDmqkuE"
      },
      "outputs": [],
      "source": [
        "# building the sequentila model and functional model\n",
        "input = keras.Input(shape = (3,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4XqsQThhb48"
      },
      "outputs": [],
      "source": [
        "# layer 1\n",
        "l1 = layers.Dense(5,activation = 'relu')\n",
        "f1 = l1(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtkjmjG4iIik"
      },
      "outputs": [],
      "source": [
        "# layer 2\n",
        "f2 = layers.Dense(5,activation='relu')(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DdiHZUeidm3"
      },
      "outputs": [],
      "source": [
        "# layer 3 or output layer\n",
        "output = layers.Dense(1)(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5-onJtritGR"
      },
      "outputs": [],
      "source": [
        "# designing the model\n",
        "func_model = keras.Model(inputs=input, outputs = output, name = 'linear_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxbHu1h5jDge"
      },
      "outputs": [],
      "source": [
        "func_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQmHDBYAjHsy"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(func_model,'linear_func_model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM6YdnkvkVsM"
      },
      "outputs": [],
      "source": [
        "func_model.compile(optimizer='sgd',loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_WXab9ljc3I"
      },
      "outputs": [],
      "source": [
        "func_hist = func_model.fit(train_x,train_y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-mHaolEk3At"
      },
      "outputs": [],
      "source": [
        "# plot the loss for linear regression model of car price prediction\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(func_hist.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEnVKfXTlEjW"
      },
      "outputs": [],
      "source": [
        "pred_y_func = func_model.predict(test_x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veD3dEXWmgbx"
      },
      "outputs": [],
      "source": [
        "print(mean_squared_error(test_y,pred_y_func))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbEwsx8eAFUl"
      },
      "source": [
        "# Training and Evaluation on some real data set using TensorFLow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0wifN_nWWEO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.formula.api import ols\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "from pandas import DataFrame,Series\n",
        "from sklearn.preprocessing import StandardScaler #for Normalization\n",
        "from sklearn.linear_model import LinearRegression # fitting a Linear Regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXpW6EzHVv7O"
      },
      "outputs": [],
      "source": [
        "downloaded4 = drive.CreateFile({'id':'1eYl-9fv5WvTNdoD7FpBqd6euwmrULn-I'}) # replace the id with id of file you want to access\n",
        "downloaded4.GetContentFile('real_estate.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MVTvuz2WNpd"
      },
      "outputs": [],
      "source": [
        "df_real_estate = pd.read_csv('real_estate.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxk0Yqp9WZro"
      },
      "outputs": [],
      "source": [
        "df_real_estate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtmS7T_qWhhB"
      },
      "outputs": [],
      "source": [
        "df_real_estate.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jer0zLvNWmI6"
      },
      "outputs": [],
      "source": [
        "# dropping some useless colums to reduce the dimesions\n",
        "drop_cols = ['No','X1 transaction date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqBdPJaDW0Dx"
      },
      "outputs": [],
      "source": [
        "df_real_estate1 = df_real_estate.copy() # creating a deep copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJAvF0ITYt4X"
      },
      "outputs": [],
      "source": [
        "df_real_estate1 = df_real_estate1.drop(columns = drop_cols,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UAp9eGZZCe3"
      },
      "outputs": [],
      "source": [
        "df_real_estate1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qJ2SkYPZGMG"
      },
      "outputs": [],
      "source": [
        "df_real_estate1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNvIoIZyZIUO"
      },
      "outputs": [],
      "source": [
        "y = df_real_estate1['Y house price of unit area']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnktV3YiZQJY"
      },
      "outputs": [],
      "source": [
        "# lets look at the scatter plot to see the postive or negative dispersion\n",
        "sns.scatterplot(x = df_real_estate1['X2 house age'],y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0IVp-R8cKe1"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x = df_real_estate1['X3 distance to the nearest MRT station'],y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwCuuli1cS5w"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x = df_real_estate1['X4 number of convenience stores'],y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3afXRumca8_"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x = df_real_estate1['X5 latitude'],y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYtXg6VUcgmf"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x = df_real_estate1['X6 longitude'],y=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyJN90LwckVJ"
      },
      "outputs": [],
      "source": [
        "df_real_estate1.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-hWVrMLczfY"
      },
      "outputs": [],
      "source": [
        "# as we can see that only three variables are giving a moderate correleation with\n",
        "# the target value so we will consider only these three variables\n",
        "df_real_estate1 = df_real_estate1.drop(columns='Y house price of unit area',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFvqyn4CbzKv"
      },
      "outputs": [],
      "source": [
        "# before fitting into the model it is important to caluculate VIF Factor\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "x = df_real_estate1 \n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = ['house age','distance to the nearest MRT station','number of convenience stores','latitude','longitude']\n",
        "vif_data['VIF'] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]\n",
        "vif_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WyP568meEYc"
      },
      "outputs": [],
      "source": [
        "# normalizing the data \n",
        "df_real_estate1['house_price'] = y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbwTIe0IeTa1"
      },
      "outputs": [],
      "source": [
        "sc_real_estate = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HptIuIqkeeVt"
      },
      "outputs": [],
      "source": [
        "real_estate_norm = sc_real_estate.fit_transform(df_real_estate1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQcaDuDXe4lF"
      },
      "outputs": [],
      "source": [
        "df_real_estate_norm = DataFrame(columns=['X2','X3','X4','X5','X6','Y'],data=real_estate_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JOC1O3blAul"
      },
      "outputs": [],
      "source": [
        "df_real_estate_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdv-pTsmeryx"
      },
      "outputs": [],
      "source": [
        "y_norm = df_real_estate_norm['Y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okd1_sptfXDP"
      },
      "outputs": [],
      "source": [
        "x_norm = df_real_estate_norm.iloc[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7BMIdWLle4X"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdj0DIeOfgg_"
      },
      "outputs": [],
      "source": [
        "# we will use Function model API of Keras to build this model\n",
        "input_house = keras.Input(shape=(5,)) # creating the input layer of 5 elts 1D tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K2llln5gMKZ"
      },
      "outputs": [],
      "source": [
        "# First Functional Layer\n",
        "l1 = layers.Dense(10,activation = 'relu')\n",
        "f1 = l1(input_house)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O8bWMT7g9RS"
      },
      "outputs": [],
      "source": [
        "# second Layer\n",
        "f2 = layers.Dense(6,activation='relu')(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BYILbRUhcpo"
      },
      "outputs": [],
      "source": [
        "output_house = layers.Dense(1)(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "620S9P9hhvFL"
      },
      "outputs": [],
      "source": [
        "model_real_estate = keras.Model(inputs=input_house,outputs=output_house,name='real_estate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TMe8hvHhvCj"
      },
      "outputs": [],
      "source": [
        "model_real_estate.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgk89nWshu_2"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model_real_estate,'linear_func_model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEnZUetGhu88"
      },
      "outputs": [],
      "source": [
        "model_real_estate.compile(optimizer='sgd',loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0zgMUdFjkP4"
      },
      "outputs": [],
      "source": [
        "# model is ready let us now create train and test data\n",
        "train_real_x,test_real_x,train_real_y,test_real_y = train_test_split(x_norm,y_norm,test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WNz6zj9jc5f"
      },
      "outputs": [],
      "source": [
        "func_hist_house = model_real_estate.fit(train_real_x,train_real_y,epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AW8JE03jc26"
      },
      "outputs": [],
      "source": [
        "# plot the loss for linear regression model of car price prediction\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(func_hist_house.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RA5EJdZkUkM"
      },
      "outputs": [],
      "source": [
        "pred_y_func_house = model_real_estate.predict(test_real_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8D8fk8DkbCj"
      },
      "outputs": [],
      "source": [
        "print(mean_squared_error(test_real_y,pred_y_func_house))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C2z4x1dSHNa"
      },
      "source": [
        "# Classification models using TensorFLow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyYEauy6APE-"
      },
      "outputs": [],
      "source": [
        "# building a classification model a binary classification model using tensor flow\n",
        "# we going to take breast cancer data from kaggle and build a model for that\n",
        "downloaded5 = drive.CreateFile({'id':'1g6y7RrQEngkDi55eQo7PlN3iE7UkjZ6r'}) # replace the id with id of file you want to access\n",
        "downloaded5.GetContentFile('breast_cancer.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDZeMSlpPIAT"
      },
      "outputs": [],
      "source": [
        "# reading breast cancer .csv file\n",
        "df_cancer = pd.read_csv('breast_cancer.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG8wkl_3PVwx"
      },
      "outputs": [],
      "source": [
        "df_cancer['diagnosis'] = df_cancer['diagnosis'].map({'M':0,'B':1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPMALiXwRR3u"
      },
      "outputs": [],
      "source": [
        "df_cancer1 = df_cancer.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhDqEY1ZQbEd"
      },
      "outputs": [],
      "source": [
        "y = df_cancer1['diagnosis'] # target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVRkxIcBrKzw"
      },
      "outputs": [],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1NAKI-1QlLm"
      },
      "outputs": [],
      "source": [
        "df_cancer1 = df_cancer1.drop(columns=['diagnosis','id'],axis=1) # dropping the target value "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aOw4CRYQwJE"
      },
      "outputs": [],
      "source": [
        "sc_cancer = StandardScaler() # intializing the normalizer to normalize the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p5ZxDIkRiKw"
      },
      "outputs": [],
      "source": [
        "cancer1_values = sc_cancer.fit_transform(df_cancer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow3K5c99Rsu5"
      },
      "outputs": [],
      "source": [
        "df_cancer_norm = DataFrame(columns=df_cancer1.columns,data=cancer1_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKkeKftErRhP"
      },
      "outputs": [],
      "source": [
        "df_cancer_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAIOdYa2R7-o"
      },
      "outputs": [],
      "source": [
        "df_cancer_norm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzPEDio-S4hM"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df_cancer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdpcFPDlSQwC"
      },
      "outputs": [],
      "source": [
        "x_can_norm = df_cancer_norm.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKPch6I3Sfeq"
      },
      "outputs": [],
      "source": [
        "# now we have data we will slowly increase the size of the models and see the effect on accuracy \n",
        "# for that we will also build confusion matrix\n",
        "# we will also build functional model API for tensorflow\n",
        "input_cancer = keras.Input(shape=(30,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "449e0da2UJLw"
      },
      "outputs": [],
      "source": [
        "l1 = layers.Dense(10,activation='relu')\n",
        "f1 = l1(input_cancer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGSlicTiUdbu"
      },
      "outputs": [],
      "source": [
        "f2 = layers.Dense(10,activation = 'relu')(f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7hKO0duUnRA"
      },
      "outputs": [],
      "source": [
        "f3 = layers.Dense(6,activation = 'relu')(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1jkD-CJUunI"
      },
      "outputs": [],
      "source": [
        "output_can = layers.Dense(1,activation='sigmoid')(f3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R71Qw86DVZZk"
      },
      "outputs": [],
      "source": [
        "can_model = keras.Model(inputs = input_cancer,outputs = output_can,name='breast_cancer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2vPM_mBV68D"
      },
      "outputs": [],
      "source": [
        "can_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIeEKBHgV-Lk"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(can_model,'can_model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQzFXsYnWZHc"
      },
      "outputs": [],
      "source": [
        "# spltting the data into two parts\n",
        "train_can_x,test_can_x,train_can_y,test_can_y = train_test_split(x_can_norm,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIjvNXlcXBq9"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "can_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3s2yB89WQ5T"
      },
      "outputs": [],
      "source": [
        "hist_can = can_model.fit(train_can_x,train_can_y,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_Jg4RROr9mx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(hist_can.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGz_4iFvsNTy"
      },
      "outputs": [],
      "source": [
        "pred_y_cancer = can_model.predict(test_can_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU-IyhsKsY1s"
      },
      "outputs": [],
      "source": [
        "pred_y_cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8y0zEL9vP0g"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIlAmF12u4Li"
      },
      "outputs": [],
      "source": [
        "encoded_y = [math.ceil(val) if val>=0.5 else math.floor(val) for val in pred_y_cancer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBtaQeTVvnIp"
      },
      "outputs": [],
      "source": [
        "print('mis classified samples %d' % (test_can_y!= encoded_y).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZUUZ3zbwUO9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(confusion_matrix(test_can_y,encoded_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhelnKigxQ0u"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_can_y,encoded_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfYKOu5yIvS8"
      },
      "source": [
        "# saving and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26AUyPTbNkFu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential,models\n",
        "from tensorflow.keras.layers import Dense,Flatten\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Tensorflow\",tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ-qBtCtIzYK"
      },
      "outputs": [],
      "source": [
        "# defining a model inside the function\n",
        "def get_model():\n",
        "  # create a simple functional model\n",
        "  input = keras.Input(shape=(3,))\n",
        "  layer1 = layers.Dense(5,activation='relu')(input)\n",
        "  layer2 = layers.Dense(5,activation = 'relu')(layer1)\n",
        "  output = layers.Dense(1)(layer2)\n",
        "  model_car = keras.Model(inputs=input,outputs=output)\n",
        "  model_car.compile(optimizer='adam',loss='mean_squared_error')\n",
        "  return model_car\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Ku7pIfI0Cv"
      },
      "outputs": [],
      "source": [
        "my_model = get_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDD87j31PsSS"
      },
      "outputs": [],
      "source": [
        "my_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5389cbuvIz_L"
      },
      "outputs": [],
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(xx1,y_norm,test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmdxXm3FPjii"
      },
      "outputs": [],
      "source": [
        "train_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agoKQbNLIz9G"
      },
      "outputs": [],
      "source": [
        "# fitting the model\n",
        "hist_car = my_model.fit(train_x,train_y,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RISW46S12pzA"
      },
      "outputs": [],
      "source": [
        "#plotting the loss function\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Loss Estimation')\n",
        "plt.plot(hist_car.history['loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wsb_72e6QKJj"
      },
      "outputs": [],
      "source": [
        "# saving the model\n",
        "# saving the model need acess to google drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT2YMsRNSC2o"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/TensorFlow/first_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0EopcyrQw-d"
      },
      "outputs": [],
      "source": [
        "# the model my_model\n",
        "my_model.save(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7SnMwCRRsi4"
      },
      "outputs": [],
      "source": [
        "# loading the model again\n",
        "reconstruct_model = keras.models.load_model(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRD1pFJ3SiGD"
      },
      "outputs": [],
      "source": [
        "# let us check wether the reconstructed model perform exactly \n",
        "# in the same way as the original one\n",
        "# let's check\n",
        "# this code will execute only when the two models will behave equally\n",
        "np.testing.assert_allclose(my_model.predict(test_x),reconstruct_model.predict(test_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2s1cOXKTXir"
      },
      "outputs": [],
      "source": [
        "# the recontructed model is ready to compile and execute \n",
        "reconstruct_model.fit(train_x,train_y,epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCtCNzeIeYiH"
      },
      "source": [
        "Handling customized objects by the saved models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6umEmFICefdP"
      },
      "outputs": [],
      "source": [
        "class customModel(keras.Model):\n",
        "  def __init__(self, hidden_neurons):\n",
        "    super(customModel,self).__init__()\n",
        "    self.hidden_neurons = hidden_neurons\n",
        "    self.dense_layers = [keras.layers.Dense(hn) for hn in hidden_neurons] # making no. of hidden layers\n",
        "  def call(self,inputs):\n",
        "    x = inputs(x)\n",
        "    for layer in self.dense_layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "  def get_config(self):\n",
        "    return {\"hidden_neurons\" : self.hidden_neurons}\n",
        "  @classmethod\n",
        "  def from_config(cls,config):\n",
        "    return cls(**config)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlkNT2OWmkmL"
      },
      "outputs": [],
      "source": [
        "# build the model by calling the contructor of the class\n",
        "model_custom = customModel([10,10,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P_GjiiYo0iB"
      },
      "outputs": [],
      "source": [
        "model_custom.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGiBHezVnLvF"
      },
      "outputs": [],
      "source": [
        "# input handling\n",
        "input_arr = tf.random.uniform((1,5))\n",
        "\n",
        "outputs = model_custom(input_arr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ITzB1IjO1z"
      },
      "source": [
        "#Preprocessing layer in TensorFLow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj4onF77CSCU"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pGAZ73SCWOl"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYxLKwp-CeL1"
      },
      "outputs": [],
      "source": [
        "downloaded5 = drive.CreateFile({'id':'1exm-YLo7NqThHHzIQoVi3J2nxuD9qGWL'}) # replace the id with id of file you want to access\n",
        "downloaded5.GetContentFile('heart_disease.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fuxlwi5ZjOZh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "data  = np.array([[12,34,56,57],[23,43,24,67],[45,33,32,12],[65,45,33,23]])\n",
        "layer = layers.Normalization()\n",
        "layer.adapt(data)\n",
        "normalized_data = layer(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9RtMcs4ygG-"
      },
      "outputs": [],
      "source": [
        "normalized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TUc6ROByurj"
      },
      "outputs": [],
      "source": [
        "# similary we can now use our data sets we we use above and will do normalization\n",
        "df_heart = pd.read_csv('heart_disease.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-58WrI2vIXBc"
      },
      "outputs": [],
      "source": [
        "df_heart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UytNKZWCyrZ"
      },
      "outputs": [],
      "source": [
        "# continuos variable\n",
        "cont_var = ['RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']\n",
        "df_heart1 = df_heart[cont_var]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN9KwAT_Imze"
      },
      "outputs": [],
      "source": [
        "df_heart1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0OLci-7C9hS"
      },
      "outputs": [],
      "source": [
        "# designing a normalized layer for the continuos data\n",
        "layer_heart = layers.Normalization()\n",
        "layer_heart.adapt(df_heart1)\n",
        "heart_data_norm = layer_heart(df_heart1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NOu1dJADyzx"
      },
      "outputs": [],
      "source": [
        "x = np.array(heart_data_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdrlEDEGFQer"
      },
      "outputs": [],
      "source": [
        "y_heart = df_heart['HeartDisease']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYJBlcCTFUPr"
      },
      "outputs": [],
      "source": [
        "y_heart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E13OouJBDfkB"
      },
      "outputs": [],
      "source": [
        "# creating  tensorflow model\n",
        "input_heart = keras.Input(shape=(5,))\n",
        "l1 = layers.Dense(10,activation='relu')\n",
        "f1 = l1(input_heart)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oG2R6y3EMzg"
      },
      "outputs": [],
      "source": [
        "f2 = layers.Dense(10,activation = 'relu')(f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrm9E1BYEPSR"
      },
      "outputs": [],
      "source": [
        "f3 = layers.Dense(6,activation = 'relu')(f2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XdH6dr4EaPx"
      },
      "outputs": [],
      "source": [
        "output_heart = layers.Dense(1,activation='sigmoid')(f3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmEmzLOXEl1a"
      },
      "outputs": [],
      "source": [
        "heart_model = keras.Model(inputs = input_heart,outputs = output_heart,name='heart_disease')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWN7-wmlEquE"
      },
      "outputs": [],
      "source": [
        "heart_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNVusgGHEywD"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(heart_model,'heart_model.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC_79oyEEayL"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "heart_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5qBgJXJE34b"
      },
      "outputs": [],
      "source": [
        "# spltting the data into two parts\n",
        "train_hrt_x,test_hrt_x,train_hrt_y,test_hrt_y = train_test_split(x,y_heart,test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i7A1E0PGOYG"
      },
      "outputs": [],
      "source": [
        "heart_model.fit(train_hrt_x,train_hrt_y,epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPqPClQfJQb-"
      },
      "outputs": [],
      "source": [
        "pred_hrt = heart_model.predict(test_hrt_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEPOdQ37JZgv"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxrWZd5NJFGX"
      },
      "outputs": [],
      "source": [
        "encoded_y = [math.ceil(val) if val>=0.5 else math.floor(val) for val in pred_hrt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY4EuvlcJb9X"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(confusion_matrix(test_hrt_y,encoded_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7KuqqG7XKl3"
      },
      "source": [
        "# Implementing Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V86oDRJCsKpo"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dmVQu0dsNY6"
      },
      "outputs": [],
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUF1V71GXmPv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,MultiLabelBinarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FUeDrDgX_Cg"
      },
      "source": [
        "Binary Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLlcj7kXX-jI"
      },
      "outputs": [],
      "source": [
        "# let us take Heart Disease data to compute Binary Cross Entropy\n",
        "downloaded5 = drive.CreateFile({'id':'1exm-YLo7NqThHHzIQoVi3J2nxuD9qGWL'}) # replace the id with id of file you want to access\n",
        "downloaded5.GetContentFile('heart_disease.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flIdDU0pZ33H"
      },
      "outputs": [],
      "source": [
        "df_heart = pd.read_csv('heart_disease.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6txpIZiHZ8-u"
      },
      "outputs": [],
      "source": [
        "df_heart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnBRUpMGaBiI"
      },
      "outputs": [],
      "source": [
        "# choose two any two continuos values along  with the target values to calulate the \n",
        "# binary cross entropy\n",
        "df_new_hrt = df_heart.loc[:,['RestingBP','Cholesterol']]\n",
        "y = df_heart['HeartDisease']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "navFvb7cahqC"
      },
      "outputs": [],
      "source": [
        "# step 1 we train the model with the train data \n",
        "# step 2 then we see the differnce between the probailistic value \n",
        "# after first epoch\n",
        "# and keep on checking the binary cross entropy after every epoch\n",
        "# but we do normalization of data first\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(df_new_hrt)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9jFuX33ac7y"
      },
      "outputs": [],
      "source": [
        "x = np.array(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz7ZNqC3d4xy"
      },
      "outputs": [],
      "source": [
        "# spltting the data into two parts\n",
        "train_hrt_x,test_hrt_x,train_hrt_y,test_hrt_y = train_test_split(x,y,test_size=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG9gb2JjbYMw"
      },
      "source": [
        "Build the model for calculating the binary cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh-A0xQ7bX_I"
      },
      "outputs": [],
      "source": [
        "input_val = keras.Input(shape=(2,))\n",
        "layer1 = layers.Dense(5,activation='relu')(input_val)\n",
        "output = layers.Dense(1,activation='sigmoid')(layer1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVfFUT9jcG-t"
      },
      "outputs": [],
      "source": [
        "# building the model\n",
        "model_bce = keras.Model(inputs = input_val,outputs = output,name = 'bce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQeWHVlVcd-i"
      },
      "outputs": [],
      "source": [
        "model_bce.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0sijyvLch-q"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model_bce,'bce.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NKuh981exfp"
      },
      "outputs": [],
      "source": [
        "from math import log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTgpbtuUcrKU"
      },
      "outputs": [],
      "source": [
        "model_bce.compile(optimizer='adam',loss='binary_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLzmUNIyfZ7t"
      },
      "outputs": [],
      "source": [
        "bce_list = []\n",
        "for epoch in range(10):\n",
        "  model_bce.fit(train_hrt_x,train_hrt_y,epochs=epoch)\n",
        "  y_pred = model_bce.predict(test_hrt_x)\n",
        "  log_pred_y = [log(val) for val in y_pred]\n",
        "  bce = -(1/len(test_hrt_y))*sum(log_pred_y)\n",
        "  bce_list.append(bce)\n",
        "  print(bce)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Plh5x5eSqS"
      },
      "outputs": [],
      "source": [
        "#plotting the binary cross entropies \n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Binary Cross Estimation')\n",
        "plt.plot(bce_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl1cXi0recqg"
      },
      "outputs": [],
      "source": [
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzrKoQmSXOWg"
      },
      "source": [
        "Categorical Cross Enropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IHEXc1OHIvf"
      },
      "outputs": [],
      "source": [
        "# computing categorical cross entropy\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2wmErDtXR1-"
      },
      "outputs": [],
      "source": [
        "downloaded = drive.CreateFile({'id':'14HS533BGJK0XtfsdYQTa9_LeBRjZg5EM'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('text_classification.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6nNQwOzXd64"
      },
      "outputs": [],
      "source": [
        "df_text = pd.read_csv('text_classification.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rulOtVKqEVM0"
      },
      "outputs": [],
      "source": [
        "df_text.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t--SGNypdvCg"
      },
      "outputs": [],
      "source": [
        "df_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmppaluxC0J4"
      },
      "outputs": [],
      "source": [
        "# one hot encoder take onlu numerical categorical value fro one hot encoding\n",
        "# creating instance of one hot encoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_fqBEApDOJO"
      },
      "outputs": [],
      "source": [
        "ohe_df = pd.DataFrame(ohe.fit_transform(df_text[['label']]).toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-MbmxdJE_zx"
      },
      "outputs": [],
      "source": [
        "y = df_text['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0s2xuZJFLUc"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdujrTycD0_L"
      },
      "outputs": [],
      "source": [
        "y_text = ohe_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6ZdDyYZd4G_"
      },
      "outputs": [],
      "source": [
        "y_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppSFN8I5E2xU"
      },
      "outputs": [],
      "source": [
        "x = df_text.iloc[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwFCmzd8EDx-"
      },
      "outputs": [],
      "source": [
        "# build a tensor flow classifier for multi class network\n",
        "input_text = keras.Input(shape=(12,))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5mvhzEJEdzt"
      },
      "outputs": [],
      "source": [
        "layer1 = layers.Dense(6,activation='relu')(input_text)\n",
        "layer2 = layers.Dense(6,activation='relu')(layer1)\n",
        "output = layers.Dense(6,activation='softmax')(layer2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGoJXjp9FV_x"
      },
      "outputs": [],
      "source": [
        "model_text = keras.Model(inputs = input_text,outputs = output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRQgorr2FwIv"
      },
      "outputs": [],
      "source": [
        "train_txt_x,test_txt_x,train_txt_y,test_txt_y = train_test_split(x,y_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMSiZtu9Gybd"
      },
      "outputs": [],
      "source": [
        "model_text.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdDyCZjEGArq"
      },
      "outputs": [],
      "source": [
        "# fitting the model\n",
        "# for differnt epochs to check differnt cross entropies\n",
        "cee_list = [] # for list of differnt cross enropies for different epochs\n",
        "for epoch in range(10):\n",
        "  model_text.fit(train_txt_x,train_txt_y,epochs=epoch,verbose=2)\n",
        "  y_pred_txt = model_text.predict(test_txt_x)\n",
        "  cce = CategoricalCrossentropy()\n",
        "  cross_entropy = [cce(test_txt_y[i],y_pred_txt[i]).numpy() for i in range(len(test_txt_y))]\n",
        "  total_cce = (1/len(test_txt_y))*sum(cross_entropy)\n",
        "  cee_list.append(total_cce)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7wF8s85JOGY"
      },
      "outputs": [],
      "source": [
        "#plotting the categorical cross entropies\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('categorical entopy')\n",
        "plt.plot(cee_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQqidV_yXz-Q"
      },
      "source": [
        "# Implementation of Sparse Categorical Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtyRyzzYXy9V"
      },
      "outputs": [],
      "source": [
        "# in sparse categorical entropy we have two types of encoding mechanism for integer encoding\n",
        "# of the categorical data, label encoding and ordinal encoding\n",
        "# earlier label encoding was used for encoding only target values \n",
        "# and ordinal encoding was used for encoding data frames that is the reason\n",
        "# labelEncoding has(n_samples) and ordinalEncoding has(n_samples,n_features) as \n",
        "# parameters\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "scc = SparseCategoricalCrossentropy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSE9JJ-ueCbL"
      },
      "outputs": [],
      "source": [
        "y_sce = df_text['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkLF41vVghIK"
      },
      "outputs": [],
      "source": [
        "y_sce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zMLF5OGeH7B"
      },
      "outputs": [],
      "source": [
        "x_sce = df_text.iloc[:,:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKpA4PtGdKZA"
      },
      "outputs": [],
      "source": [
        "train_txt_x_sce,test_txt_x_sce,train_txt_y_sce,test_txt_y_sce = train_test_split(x_sce,y_sce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkU9ckREeq0s"
      },
      "outputs": [],
      "source": [
        "#setting the input value\n",
        "input_text = keras.Input(shape=(12,))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAmY_c4Peaku"
      },
      "outputs": [],
      "source": [
        "# building a FUnctional Keras Model\n",
        "layer1 = layers.Dense(6,activation='relu')(input_text)\n",
        "layer2 = layers.Dense(6,activation='relu')(layer1)\n",
        "output = layers.Dense(6,activation='softmax')(layer2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_02Gwrn4ev9T"
      },
      "outputs": [],
      "source": [
        "model_text_sce = keras.Model(inputs = input_text,outputs = output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SB_60UVe011"
      },
      "outputs": [],
      "source": [
        "model_text_sce.compile(loss='sparse_categorical_crossentropy',optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZC38vLphfBd"
      },
      "outputs": [],
      "source": [
        "model_text_sce.fit(train_txt_x_sce,train_txt_y_sce,epochs=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYBBAxkCe9Cx"
      },
      "outputs": [],
      "source": [
        "# fitting the model\n",
        "# for differnt epochs to check differnt kl divergence\n",
        "sce_list = [] # for list of differnt KL divergence for different epochs\n",
        "for epoch in range(10):\n",
        "  model_text_sce.fit(train_txt_x_sce,train_txt_y_sce,epochs=epoch)\n",
        "  y_pred_txt_sce = model_text.predict(test_txt_x_sce)\n",
        "  sce = SparseCategoricalCrossentropy()\n",
        "  sce_val = [sce(test_txt_y_sce[i],y_pred_txt_sce[i]).numpy() for i in range(len(test_txt_y_sce))]\n",
        "  total_sce = (1/len(test_txt_y_sce))*sum(sce_val)\n",
        "  sce_list.append(total_sce)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11Hg3H7T0oaD"
      },
      "outputs": [],
      "source": [
        "sce_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCg3X6aviE8b"
      },
      "outputs": [],
      "source": [
        "#plotting the categorical cross entropies\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('Sparse Categorical Entropy')\n",
        "plt.plot(sce_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw3AcUfJq4ik"
      },
      "source": [
        "Kullback-Leibler KL(P||Q) divergence is used to measure the difference between the two distribution to measure the diffence between two distribution we will use text classification data which is a multiclass data\n",
        "\n",
        "KL Divegence is a metric so we need a loss function we going to use categorical cross entropy and then use a function of kl divergence for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se6anprmKSpq"
      },
      "outputs": [],
      "source": [
        "downloaded = drive.CreateFile({'id':'14HS533BGJK0XtfsdYQTa9_LeBRjZg5EM'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('text_classification.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lF4_TuliamO"
      },
      "outputs": [],
      "source": [
        "df_text = pd.read_csv('text_classification.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZQI0AjGul76"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "def kl_div(x,y):\n",
        "  return -sum([x[i]*log(y[i]/x[i]) for i in range(len(x))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_yT7PFCvVM8"
      },
      "outputs": [],
      "source": [
        "# define two dummy distributions\n",
        "p = [0.36,0.52,0.63,0.45,0.15,0.56]\n",
        "q = [0.15,0.37,0.78,0.51,0.37,0.50]\n",
        "\n",
        "kl_pq = kl_div(p,q)\n",
        "print(\"KL(p||q) is %.3f bits\"%kl_pq)\n",
        "\n",
        "kl_qp = kl_div(q,p)\n",
        "print(\"KL(q||p) is %.3f bits\"%kl_pq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69HzRCTwsmSp"
      },
      "outputs": [],
      "source": [
        "# from the above implementation of categrical cross entropy we are using the same model but now \n",
        "# after every cycle of loop which is Varying number of epochs will be calling KL Divergence functions\n",
        "from tensorflow.keras.losses import KLDivergence\n",
        "\n",
        "# fitting the model\n",
        "# for differnt epochs to check differnt kl divergence\n",
        "kl_list = [] # for list of differnt KL divergence for different epochs\n",
        "for epoch in range(10):\n",
        "  model_text.fit(train_txt_x,train_txt_y,epochs=epoch)\n",
        "  y_pred_txt = model_text.predict(test_txt_x)\n",
        "  kl_div = KLDivergence()\n",
        "  kl_values = [kl_div(test_txt_y[i],y_pred_txt[i]).numpy() for i in range(len(test_txt_y))]\n",
        "  total_kl_div = (1/len(test_txt_y))*sum(kl_values)\n",
        "  kl_list.append(total_kl_div)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH_xYnktzXC6"
      },
      "outputs": [],
      "source": [
        "#plotting the categorical cross entropies\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('number of epoch')\n",
        "plt.ylabel('KL Divergence Values')\n",
        "plt.plot(kl_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36L16y14BXg9"
      },
      "source": [
        "# Intoduction to Computer Vision for TensorFLow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGR0xHntW0ij"
      },
      "source": [
        "Training a Fashion MNIST with Callbacks function to control training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhOx6afbW-zb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers.advanced_activations import ReLU\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSR7M2fpP5rA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(train_im,train_lbl),(test_im,test_lbl) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iK0B98kTyDQQ"
      },
      "outputs": [],
      "source": [
        "train_im = train_im.reshape((60000,28*28))\n",
        "test_im = test_im.reshape((10000,28*28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jYwJpjJyXv0"
      },
      "outputs": [],
      "source": [
        "#normalization pixels values in the range of 0.0 to 1.0 by dividing with 255.0\n",
        "train_im = train_im/255.0\n",
        "test_im = test_im/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReM4wjhkx5jt"
      },
      "outputs": [],
      "source": [
        "train_im.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLV39w6AzQ6L"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('loss')<0.5):\n",
        "      print(\"now loss has been optimized stop training\")\n",
        "      self.model.stop_training  =True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmzrBYvMZ9AC"
      },
      "outputs": [],
      "source": [
        "callbacks = myCallback()\n",
        "model_f = Sequential()\n",
        "model_f.add(Dense(128,activation='relu',input_shape=(28*28,)))\n",
        "model_f.add(Dense(10,activation='softmax'))\n",
        "model_f.compile(optimizer='adam',loss=loss_fn,metrics=['accuracy'])\n",
        "model_f.fit(train_im,train_lbl,epochs=10,callbacks=[callbacks])\n",
        "#model.evaluate(test_im,test_lab)\n",
        "#model_f.predict(test_im)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uthzl7GebJp6"
      },
      "outputs": [],
      "source": [
        "model_f.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYprPMzf0CiJ"
      },
      "source": [
        "Make a checkpoint of model and implementing tranfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ5PsbGa0GaK"
      },
      "outputs": [],
      "source": [
        "!pip install h5py pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uxq1seSm0RdW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y69BTnYl0eo_"
      },
      "outputs": [],
      "source": [
        "(train_img,train_lbl),(test_img,test_lbl) = tf.keras.datasets.mnist.load_data()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9d27HfV1BbV"
      },
      "outputs": [],
      "source": [
        "train_img = train_img/255.0\n",
        "test_img = test_img/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-sMuMBe1swz"
      },
      "outputs": [],
      "source": [
        "def classify_model():\n",
        "  model_mnist = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                            tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                            tf.keras.layers.Dense(10,activation='softmax')\n",
        "                                            \n",
        "  ])\n",
        "  model_mnist.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam',metrics=['accuracy'])\n",
        "  return model_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHwCZEso4GL8"
      },
      "outputs": [],
      "source": [
        "model_mn = classify_model()\n",
        "model_mn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW-BIRt6414i"
      },
      "outputs": [],
      "source": [
        "# create a check point path\n",
        "# saving the model\n",
        "# saving the model nees acess to google drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E57r_-5w5WsG"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/TensorFlow/cp_mnist.ckpt'\n",
        "checkpoint_dir = os.path.dirname(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nAQ1LKv6VYb"
      },
      "outputs": [],
      "source": [
        "# create a check point call back\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(path, save_weights_only=True,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67S5W_yZ6s8o"
      },
      "outputs": [],
      "source": [
        "model_mn.fit(train_img,train_lbl,epochs=10,validation_data=(test_img,test_lbl),\n",
        "             callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl2_vRCv6s4n"
      },
      "outputs": [],
      "source": [
        "# now lets run the mnist with untrained model\n",
        "model2 = classify_model() # calling the function to create the model\n",
        "loss,acc = model2.evaluate(test_img,test_lbl)\n",
        "print(\"the accuracy is %.3f and loss is %.3f\"%(acc,loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4AyatYt_Qo7"
      },
      "outputs": [],
      "source": [
        "# now this can we seems to be very poor performance model we have \n",
        "# two options either train again or use orevious trained checpointed model\n",
        "# we go for second Option !!!!!\n",
        "model2.load_weights(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDg5TnCp_scF"
      },
      "outputs": [],
      "source": [
        "loss,acc = model2.evaluate(test_img,test_lbl)\n",
        "print(\"After using the checkpointed model, the accuracy is %.3f and loss is %.3f\"%(acc,loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeXtGHExvbNQ"
      },
      "source": [
        "Embedding layer in NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrWATt6fvaz5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLjMuMQpPDCY"
      },
      "outputs": [],
      "source": [
        "w1 = tf.compat.v1.get_variable('W1',[100,50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCBqqNTsPI05"
      },
      "outputs": [],
      "source": [
        "b1 = tf.compat.v1.get_variable('b1',[20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWliKJr-Pb2q"
      },
      "outputs": [],
      "source": [
        "w2 = tf.compat.v1.get_variable('W2',[17,20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT2QEBGyPpzJ"
      },
      "outputs": [],
      "source": [
        "b2 = tf.compat.v1.get_variable('b2',[17])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoGgaeWoOXl4"
      },
      "outputs": [],
      "source": [
        "look_up = tf.compat.v1.get_variable('W',[100,50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6kfimQyx34u"
      },
      "outputs": [],
      "source": [
        "p1 = keras.Input(shape=(()),dtype=tf.int32,name = 'W1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMZBlknQV8yo"
      },
      "outputs": [],
      "source": [
        "p2 = keras.Input(shape=(()),dtype=tf.int32,name = 'W2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGZ5QzccWF75"
      },
      "outputs": [],
      "source": [
        "p3 = keras.Input(shape=(()),dtype=tf.int32,name = 'W3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKIil9a1zaLR"
      },
      "outputs": [],
      "source": [
        "v_w1 = tf.nn.embedding_lookup(look_up,p1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2amMpIqzQ9RF"
      },
      "outputs": [],
      "source": [
        "v_w2 = tf.nn.embedding_lookup(look_up,p2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juzUr9pDWRtL"
      },
      "outputs": [],
      "source": [
        "v_w3 = tf.nn.embedding_lookup(look_up,p3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITgBS3eUXvr-"
      },
      "outputs": [],
      "source": [
        "x = tf.concat([v_w1,v_w2,v_w3],0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCA1hS6Su39q"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilpk32Bjbdgq"
      },
      "source": [
        "Training Loop from Scratch using TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zD9Zhr2d7rz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72zzWahde4Pl"
      },
      "source": [
        "Let us have  a equation as y = 4.0*x*x + 2.5*x + 1.0\n",
        "for x = 2.5\n",
        "y = 32.25 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovr1v8Zwc6JI"
      },
      "outputs": [],
      "source": [
        "class Mymodel(tf.Module):\n",
        "  def __init__(self, name = None):\n",
        "    super().__init__(name = name)\n",
        "    self.w1 = tf.Variable(2.0)\n",
        "    self.w2 = tf.Variable(1.5)\n",
        "    self.b = tf.Variable(0.0)\n",
        "  def __call__(self,x):\n",
        "    return self.w1*x*x + self.w2*x + self.b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0dwGVWsdxpA"
      },
      "outputs": [],
      "source": [
        "model_trn = Mymodel(\"my_first_train_mdl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnVABDU7enzD"
      },
      "outputs": [],
      "source": [
        "# checking for call\n",
        "pred_y  = model_trn(2.3).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBcWRoGz6rq8"
      },
      "outputs": [],
      "source": [
        "pred_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKA1W5_Eey6c"
      },
      "outputs": [],
      "source": [
        "def loss(true_y,pred_y):\n",
        "  return tf.reduce_mean(tf.square(true_y-pred_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ7bvkZkfzqy"
      },
      "outputs": [],
      "source": [
        "# true_y = y = 32.25\n",
        "y = 32.25\n",
        "x = 2.5\n",
        "loss_val = loss(y,model_trn(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRWDxwvPgTmD"
      },
      "outputs": [],
      "source": [
        "loss_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sKDy0YvgXXY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops.gen_state_ops import assign_sub\n",
        "def train_model(x,y,model,l_rate):\n",
        "  with tf.GradientTape() as tape:\n",
        "    z = loss(y,model_trn(x))\n",
        "  dw1,dw2,db = tape.gradient(z,[model.w1,model.w2,model.b])\n",
        "  model.w1.assign_sub(dw1*l_rate)\n",
        "  model.w2.assign_sub(dw2*l_rate)\n",
        "  model.b.assign_sub(db*l_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfOhFWKYhob0"
      },
      "outputs": [],
      "source": [
        "train_model(x,y,model_trn,0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STw2yF7ciNlP"
      },
      "outputs": [],
      "source": [
        "w1_lst = []\n",
        "w2_lst = []\n",
        "b_lst = []\n",
        "for epoch in range(10):\n",
        "  w1_lst.append(model_trn.w1.numpy())\n",
        "  w2_lst.append(model_trn.w2.numpy())\n",
        "  b_lst.append(model_trn.b.numpy())\n",
        "  train_model(x,y,model_trn,0.1)\n",
        "  pred_y = model_trn(x)\n",
        "  print(pred_y.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW7nnTr5khH5"
      },
      "outputs": [],
      "source": [
        "print(w1_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbuTHwIZquCx"
      },
      "outputs": [],
      "source": [
        "print(w2_lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK14zQ0COm92"
      },
      "source": [
        "# Convolution Neural Network using TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru8bsLy3tje4"
      },
      "source": [
        "Building basic ConvNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Jdz7bXti6K"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "model_conv = tf.keras.models.Sequential([tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)), # 64 filters of size (3,3)\n",
        "                                         tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                         tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n",
        "                                         tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                         tf.keras.layers.Flatten(),\n",
        "                                         tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                         tf.keras.layers.Dense(10,activation='softmax')\n",
        "                                         ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4g5F9I4u86K"
      },
      "outputs": [],
      "source": [
        "fn_mnist = tf.keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnPK1W35MCna"
      },
      "outputs": [],
      "source": [
        "(fn_trn_img,fn_trn_lbl),(fn_test_img,fn_test_lbl) = fn_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivzsW_RzjM60"
      },
      "outputs": [],
      "source": [
        "fn_trn_img = fn_trn_img/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knFS-9w6jfQr"
      },
      "outputs": [],
      "source": [
        "fn_test_img = fn_test_img/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POCd1DTaMS5Z"
      },
      "outputs": [],
      "source": [
        "model_conv.compile(optimizer='adam',loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTHQfBreMofu"
      },
      "outputs": [],
      "source": [
        "model_conv.fit(fn_trn_img,fn_trn_lbl,epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyJDKqvCB8gL"
      },
      "outputs": [],
      "source": [
        "loss,acc = model_conv.evaluate(fn_trn_img,fn_trn_lbl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhwIaYNgCVCi"
      },
      "source": [
        "Comparative Analysis with vanilla model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XQiumbrCxmW"
      },
      "outputs": [],
      "source": [
        "model_vanilla = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                         tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                         tf.keras.layers.Dense(10,activation='softmax')\n",
        "                                         ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHRn1grBCnmv"
      },
      "outputs": [],
      "source": [
        "(fn_trn_img_vl,fn_trn_lbl_vl),(fn_test_img_vl,fn_test_lbl_vl) = fn_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlWRTMhCjvVN"
      },
      "outputs": [],
      "source": [
        "fn_trn_img = fn_trn_img/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GSJl8dmjwA1"
      },
      "outputs": [],
      "source": [
        "fn_test_img = fn_test_img/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUtogM0SEKK8"
      },
      "outputs": [],
      "source": [
        "model_vanilla.compile(optimizer='adam',loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpZPXyk0EWoG"
      },
      "outputs": [],
      "source": [
        "model_vanilla.fit(fn_trn_img_vl,fn_trn_lbl_vl,epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REHQdQW1EphW"
      },
      "source": [
        "# Visualizing ConvNets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og7USfICbeU9"
      },
      "outputs": [],
      "source": [
        "print(fn_test_lbl[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rivTBDCb8lu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FSfqF2nhOMj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "num=20\n",
        "num_row = 4\n",
        "num_col = 5# plot images\n",
        "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
        "for i in range(num):\n",
        "    ax = axes[i//num_col, i%num_col]\n",
        "    ax.imshow(fn_test_img[i], cmap='gray')\n",
        "    ax.set_title('Label: {}'.format(fn_test_lbl[i]))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z6RZf6KtFMM"
      },
      "outputs": [],
      "source": [
        "layer_output = [layer.output for layer in model_conv.layers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXD-5Zh_2cRK"
      },
      "outputs": [],
      "source": [
        "layer_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPf66GjQVi9C"
      },
      "outputs": [],
      "source": [
        "f, axarr = plt.subplots(3,4) # for each image 4 outputs of every layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3My9RMGsV04T"
      },
      "outputs": [],
      "source": [
        "img1_idx = 1\n",
        "img2_idx = 4\n",
        "img3_idx = 6\n",
        "conv_no = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxpzzJs52iuT"
      },
      "outputs": [],
      "source": [
        "# creating the activation model\n",
        "activation_model = tf.keras.models.Model(inputs=model_conv.input, outputs=layer_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1Yz7DpXdj8H"
      },
      "outputs": [],
      "source": [
        "print(fn_test_img[img1_idx].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5idC2DWtcn_q"
      },
      "outputs": [],
      "source": [
        "activation = activation_model.predict(fn_test_img[img1_idx].reshape(1, 28, 28, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qvxW0dNeU-9"
      },
      "outputs": [],
      "source": [
        "# first output layer of activation model which consist of for o/p layers\n",
        "first_layer_activation = activation[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C86bRuVieeWO"
      },
      "outputs": [],
      "source": [
        "# shape of first layer activation\n",
        "print(first_layer_activation.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecH0eqeXeiav"
      },
      "outputs": [],
      "source": [
        "# 10th channel of the image after first layer of convolution is applied\n",
        "plt.matshow(first_layer_activation[0, :, :, 52], cmap ='gray') # axis=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjIoLr9nesjA"
      },
      "outputs": [],
      "source": [
        "# 15th channel of the image after first layer of convolution is applied\n",
        "plt.matshow(first_layer_activation[0, :, :, 15], cmap ='viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8088rjodeyG_"
      },
      "outputs": [],
      "source": [
        "# second o/p layer for image at img1_idx\n",
        "second_layer_activation = activation[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol3bbRq9CHQa"
      },
      "outputs": [],
      "source": [
        "second_layer_activation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "811H343XBWUw"
      },
      "outputs": [],
      "source": [
        "# second layer output for first image\n",
        "plt.matshow(second_layer_activation[0, :, :, 10], cmap = 'viridis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8-TlLeYDs-2"
      },
      "outputs": [],
      "source": [
        "print(second_layer_activation[0, :, :, 10]) #pixel values of feature map at 10 level i.e o/p from 10 filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyGptKTX3wRQ"
      },
      "source": [
        "# Building a real image classifier using ConvNets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6_jPPOV34fX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf # tensorflow is using keras as the back end\n",
        "print(tf.__version__)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSTQi87F0ao2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FsMqoKR-tnu"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from matplotlib import pyplot\n",
        "#define location of folder\n",
        "from numpy import asarray\n",
        "from numpy import save\n",
        "import numpy as np\n",
        "import re\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import cv2 as cv\n",
        "path1 = '/content/drive/MyDrive/dog_cat/dogdata/'\n",
        "\n",
        "img_data = []\n",
        "img_lbl = []\n",
        "for file in listdir(path1):\n",
        "  if re.match(r'd[a-zA-Z0-9\\\\.]+',file):\n",
        "    output = 0.0\n",
        "    img_lbl.append(output)\n",
        "  else:\n",
        "    output = 1.0\n",
        "    img_lbl.append(output)\n",
        "  photo  = load_img(path1+file,target_size = (150,150))\n",
        "  #photo = cv.cvtColor(np.float32(photo),cv.COLOR_BGR2GRAY)\n",
        "  # the neural n/w take the image as array\n",
        "  photo = img_to_array(photo)\n",
        "  img_data.append(photo)\n",
        "  \n",
        "train_img_data = asarray(img_data)\n",
        "train_labels = asarray(img_lbl)  \n",
        "print(train_img_data.shape)\n",
        "print(train_labels.shape)\n",
        "save('dog_cats.npy',train_img_data)\n",
        "save('dog_cats_labels.npy',train_labels)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0gYsYS9XT0_"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/dog_cat/mini_dog_cat/'\n",
        "pics,labels =[],[]\n",
        "for file in listdir(path):\n",
        "  # assign class labels\n",
        "  # taregt value zero is for dog and one is for cat\n",
        "  output = 0.0\n",
        "  if file.startswith('cat'):\n",
        "    output = 1.0\n",
        "  photo  = load_img(path+file,target_size = (150,150))\n",
        "  #photo = cv.cvtColor(np.float32(photo),cv.COLOR_BGR2GRAY)\n",
        "  # the neural n/w take the image as array\n",
        "  photo = img_to_array(photo)\n",
        "  pics.append(photo)\n",
        "  labels.append(output)\n",
        "validation_pics = asarray(pics)\n",
        "validation_labels = asarray(labels)  \n",
        "print(validation_pics.shape)\n",
        "print(validation_labels.shape)\n",
        "save('dog_cats.npy',validation_pics)\n",
        "save('dog_cats_labels.npy',validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xHosiYJDfQy"
      },
      "outputs": [],
      "source": [
        "# Now we have data set in train_img_data and train_labels in labels\n",
        "# for training cat and dog classifie\n",
        "dogs = 0\n",
        "cats = 0\n",
        "for i in labels:\n",
        "  if i==0.0:\n",
        "    dogs = dogs+1\n",
        "  else:\n",
        "    cats = cats+1\n",
        "print('dogs = {}, cats = {}'.format(dogs,cats))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVt2aw3jIP_U"
      },
      "source": [
        "Building a small convnet for Dog Vs Cat Classifier\n",
        "small because size of data is only 1000 approx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K4ibIJhIBnt"
      },
      "outputs": [],
      "source": [
        "model_cat_dog = tf.keras.models.Sequential([# stacking Convolution layers\n",
        "                                            tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',input_shape = (150,150,3)),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            # Neural netwok for classification take feature maps as O/P\n",
        "                                            tf.keras.layers.Flatten(),\n",
        "                                            tf.keras.layers.Dense(512,activation='relu'),\n",
        "                                            tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "                                            \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xSCSLHfKcNx"
      },
      "outputs": [],
      "source": [
        "model_cat_dog.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWpZQPH6KkhT"
      },
      "outputs": [],
      "source": [
        "# compiling the model\n",
        "model_cat_dog.compile(optimizer='rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Us6u5nUlNc"
      },
      "source": [
        "# Building a preporocessing layer for ConvNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrfOtVxcUeki"
      },
      "outputs": [],
      "source": [
        "train_img_data = train_img_data/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Dk-NlpYGUZ"
      },
      "outputs": [],
      "source": [
        "validation_pics = validation_pics/255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlKwn2slcHj6"
      },
      "source": [
        "Check pointing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZL9P-y0cOH7"
      },
      "outputs": [],
      "source": [
        "path_check = '/content/drive/MyDrive/Colab Notebooks/TensorFlow/cp_dogcat.ckpt'\n",
        "checkpoint_dir = os.path.dirname(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FHek-OHcgSS"
      },
      "outputs": [],
      "source": [
        "# create a check point call back\n",
        "cp_callback_dc = tf.keras.callbacks.ModelCheckpoint(path_check, save_weights_only=True,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q332hVmc1ci"
      },
      "source": [
        "Fitting the model along with checkpoint a a call back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO4QQRxCYQij"
      },
      "outputs": [],
      "source": [
        "history = model_cat_dog.fit(train_img_data,train_labels,validation_data=(validation_pics,validation_labels),epochs=10,callbacks=[cp_callback_dc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0daruFYdHO1"
      },
      "source": [
        "compare a new image classifier with the old trained one\n",
        "the new is named as model_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJaXEwFtcraK"
      },
      "outputs": [],
      "source": [
        "model_new = tf.keras.models.Sequential([# stacking Convolution layers\n",
        "                                            tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',input_shape = (150,150,3)),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            # Neural netwok for classification take feature maps as O/P\n",
        "                                            tf.keras.layers.Flatten(),\n",
        "                                            tf.keras.layers.Dense(512,activation='relu'),\n",
        "                                            tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "                                            \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D35c7dskd3tN"
      },
      "outputs": [],
      "source": [
        "# compiling the new un trained model\n",
        "model_new.compile(optimizer='rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmriDvt8djsz"
      },
      "outputs": [],
      "source": [
        "model_new.evaluate(validation_pics,validation_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9oBFPjdeEuY"
      },
      "outputs": [],
      "source": [
        "# now this can we seems to be very poor performance model we have \n",
        "# two options either train again or use orevious trained checpointed model\n",
        "# we go for second Option !!!!!\n",
        "model_new.load_weights(path_check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaizyBRBsaMV"
      },
      "outputs": [],
      "source": [
        "model_new.evaluate(validation_pics,validation_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtYNUwd3YxhI"
      },
      "source": [
        "Displaying the curve of loss and accuracy duting training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lLz_MFGaypl"
      },
      "outputs": [],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i6I53SSZRwC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(acc)+1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label = \"Accuracy_Training\")\n",
        "plt.plot(epochs, val_acc,'b', label = \"Validation_acc_training\")\n",
        "plt.title(\"Accuracy of training and Validation\")\n",
        "plt.legend()\n",
        "plt.figure()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usCGMItJaUlz"
      },
      "outputs": [],
      "source": [
        "plt.plot(epochs, loss, 'bo', label = \"Loss_Training\")\n",
        "plt.plot(epochs, val_loss,'b',label = \"Validation_Loss_training\")\n",
        "plt.title(\"Loss of training and Validation\")\n",
        "plt.legend()\n",
        "plt.figure()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdqWekD006wz"
      },
      "source": [
        "# Playing with filters of differnt types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olhd_Z721AuE"
      },
      "outputs": [],
      "source": [
        "from scipy import misc\n",
        "# load the ascent image\n",
        "rand_img = misc.ascent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD-9mXIjYr1I"
      },
      "source": [
        "Using Call Back and checkpointing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01RT6lZ21wcf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(rand_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bmGJsr506Yq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# visualize the image\n",
        "plt.grid(False)\n",
        "plt.gray()\n",
        "plt.axis('off')\n",
        "plt.imshow(rand_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqj9Xx8e2coM"
      },
      "source": [
        "The image rand_img is stored as a numpy array so you can create the transformed image by first copying it. Than we can work on each pixel of image by looping over it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3ESZoBw2b5F"
      },
      "outputs": [],
      "source": [
        "# deep copy of the image\n",
        "import numpy as np\n",
        "img_copy = np.copy(rand_img)\n",
        "\n",
        "# extract the dimensions of the image\n",
        "size_x = img_copy.shape[0]\n",
        "size_y = img_copy.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdpCcro63vPv"
      },
      "source": [
        "Now you can create filters of size 3x3 array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJze6J_Z3pzo"
      },
      "outputs": [],
      "source": [
        "#filter = [[0,1,0],[1,-4,1],[0,1,0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxolTd6A2Po0"
      },
      "outputs": [],
      "source": [
        "# a couple of more filters to try \n",
        "# these filters are been taken randomly\n",
        "filter = [[1,3,1],[0,0,0],[-1,-3,-1]]\n",
        "#filter = [[-1,0,1],[-2,0,2],[-1,0,1]]\n",
        "# filter  = [[1,2,2],[0,-1,0],[-2,0,-2]]\n",
        "# we have to notice that sum of all values of the filter should be add up to 1 or 0\n",
        "# if the sum is not equal to 1 or zero then we have to use weights to normalize them\n",
        "# now we have to multiply the filters by \n",
        "\n",
        "weight = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQe_zm6h8RQT"
      },
      "outputs": [],
      "source": [
        "# Iterate over the image\n",
        "for x in range(1, size_x-1):\n",
        "  for y in range(1, size_y-1):\n",
        "    conv = 0.0\n",
        "    conv = conv+(img_copy[x-1,y-1]*filter[0][0])\n",
        "    conv = conv+(img_copy[x-1,y]*filter[0][1])\n",
        "    conv = conv+(img_copy[x,y-1]*filter[1][0])\n",
        "    conv = conv+(img_copy[x,y]*filter[1][1])\n",
        "    conv = conv+(img_copy[x,y+1]*filter[1][2])\n",
        "    conv = conv+(img_copy[x+1,y]*filter[2][1])\n",
        "    conv = conv+(img_copy[x-1,y+1]*filter[0][2])\n",
        "    conv = conv+(img_copy[x+1,y-1]*filter[2][0])\n",
        "    conv = conv+(img_copy[x+1,y+1]*filter[2][2])\n",
        "\n",
        "    # multiply by weight = 1 because sum of values of filter is 1\n",
        "    conv = conv*weight\n",
        "\n",
        "    # check the boundary condition\n",
        "    if conv<0:\n",
        "      conv = 0\n",
        "    if conv>255:\n",
        "      conv = 255\n",
        "    # load the updated convolved value in tranformed image\n",
        "    img_copy[x,y] = conv\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zPZbNvM-v9i"
      },
      "outputs": [],
      "source": [
        "# plot the image\n",
        "plt.grid(False)\n",
        "plt.gray()\n",
        "plt.axis('off')\n",
        "plt.imshow(img_copy)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tkg4bEaBuum"
      },
      "source": [
        "Implementing Max pool layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Per-pux6BxuG"
      },
      "outputs": [],
      "source": [
        "# reducing size of pool matrix to half\n",
        "new_x = int(size_x/2)\n",
        "new_y = int(size_y/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HrysElSCCKe"
      },
      "outputs": [],
      "source": [
        "# create a blank canvas of reduced size\n",
        "poolImage = np.zeros((new_x,new_y))\n",
        "\n",
        "# Iterate over Image\n",
        "# as we have done in Convolution but this time we paste the output on \n",
        "# pooled matrix of half the size of the previous image\n",
        "for x in range(0,size_x,2):\n",
        "  #temp = []\n",
        "  for y in range(0,size_y,2):\n",
        "    temp = []\n",
        "    temp.append(img_copy[x,y])\n",
        "    temp.append(img_copy[x,y+1])\n",
        "    temp.append(img_copy[x+1,y+1])\n",
        "    temp.append(img_copy[x+1,y])\n",
        "  # get the max pixel value to canvas\n",
        "    poolImage[int(x/2),int(y/2)] = max(temp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojD_RsARDyp7"
      },
      "outputs": [],
      "source": [
        "# plot the Pooled image\n",
        "plt.grid(False)\n",
        "plt.gray()\n",
        "plt.axis('off')\n",
        "plt.imshow(poolImage)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW7G-dX2vaul"
      },
      "source": [
        "Image Data Generator for reading images from directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFx-KsqQ0Gi_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7QjoW4RzdrT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmsVKpxU0lnv"
      },
      "outputs": [],
      "source": [
        "# setting directory for training and validation \n",
        "train_path = '/content/drive/MyDrive/horse-or-human/train/'\n",
        "validation_path = '/content/drive/MyDrive/horse-or-human/validation/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7_Ka73e0wpy"
      },
      "outputs": [],
      "source": [
        "train_data_generator = train_data_gen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V7HaAb_fgqF"
      },
      "outputs": [],
      "source": [
        "test_data_generator = test_data_gen.flow_from_directory(\n",
        "    validation_path,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL7v4DCeftAP"
      },
      "outputs": [],
      "source": [
        "# let us look at the ouput of these generators\n",
        "for batch_data,batch_lbl in train_data_generator:\n",
        "  print('shape of data per batch',batch_data.shape)\n",
        "  print('shape of label per batch',batch_lbl.shape)\n",
        "  break # run for only one time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzoNzVigg1kc"
      },
      "source": [
        "Building a new model for horse and human classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21wq7C7mtpvg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_WHosYqg0J-"
      },
      "outputs": [],
      "source": [
        "model_horse_human = tf.keras.models.Sequential([# stacking Convolution layers\n",
        "                                            tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',input_shape = (150,150,3)),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n",
        "                                            tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                                            # Neural netwok for classification take feature maps as O/P\n",
        "                                            tf.keras.layers.Flatten(),\n",
        "                                            tf.keras.layers.Dense(512,activation='relu'),\n",
        "                                            tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "                                            \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV0hY8Md5gz-"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"now loss has been optimized stop training\")\n",
        "      self.model.stop_training  =True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlJIOdL634BD"
      },
      "outputs": [],
      "source": [
        "# compiling the new un trained model\n",
        "model_horse_human.compile(optimizer='rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3lBBeAEtt7H"
      },
      "outputs": [],
      "source": [
        "# fitting the model with the batch generator\n",
        "callback = myCallback()\n",
        "history  = model_horse_human.fit_generator(\n",
        "    train_data_generator,\n",
        "    #steps_per_epoch = 100,\n",
        "    epochs = 30,\n",
        "    validation_data = test_data_generator,\n",
        "    #validation_steps = 50,\n",
        "    callbacks = [callback]\n",
        "        \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu452loOY632"
      },
      "source": [
        "#Data Augmentation Via Image Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2LgEd2UZ0q3"
      },
      "outputs": [],
      "source": [
        "aug_train_datagen = ImageDataGenerator(\n",
        "    rotation_range= 45,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip= True,\n",
        "    fill_mode = 'nearest' \n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U0Hnm73ngco"
      },
      "outputs": [],
      "source": [
        "aug_test_datagen = ImageDataGenerator(rescale= 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCUPV3I1l9jG"
      },
      "outputs": [],
      "source": [
        "aug_train_data_generator = aug_train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGbt0QAspBjO"
      },
      "outputs": [],
      "source": [
        "test_data_generator = test_data_gen.flow_from_directory(\n",
        "    validation_path,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZx0JTp6pQGt"
      },
      "outputs": [],
      "source": [
        "# fitting the model with the batch generator\n",
        "callback = myCallback()\n",
        "history_aug  = model_horse_human.fit_generator(\n",
        "    aug_train_data_generator,\n",
        "    #steps_per_epoch = 100,\n",
        "    epochs = 30,\n",
        "    validation_data = test_data_generator,\n",
        "    #validation_steps = 50,\n",
        "    callbacks = [callback]\n",
        "        \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntuqlxVhq9JL"
      },
      "source": [
        "# Using some pretrained ConvNets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiKTDkHvrDd8"
      },
      "outputs": [],
      "source": [
        "# we will going to instantiate the VGG16 convolutional base\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Dense\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjNXS0ULuHob"
      },
      "outputs": [],
      "source": [
        "conv_base = VGG16( weights = 'imagenet',include_top = False, input_shape=(150,150,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uglvU8dMujdC"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ5F_iyauqD3"
      },
      "outputs": [],
      "source": [
        "model_conv_base = Sequential()\n",
        "model_conv_base.add(conv_base)\n",
        "model_conv_base.add(Flatten())\n",
        "model_conv_base.add(Dense(512,activation='relu'))\n",
        "model_conv_base.add(Dense(1,activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('accuracy')>0.98):\n",
        "      print(\"now loss has been optimized stop training\")\n",
        "      self.model.stop_training  =True"
      ],
      "metadata": {
        "id": "ZdLu-DmFN9Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv_base.compile(optimizer='rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "lkYctJ5jOA7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XPu-SBVHUmG"
      },
      "outputs": [],
      "source": [
        "callback = myCallback()\n",
        "model_conv_base.fit_generator(\n",
        "    train_data_generator,\n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    validation_data = test_data_generator,\n",
        "    callbacks = [callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Happy or Sad dataset for building a classifier"
      ],
      "metadata": {
        "id": "1_NEega9OdhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from tensorflow.keras.optimizer import RMSprop\n",
        "import os\n"
      ],
      "metadata": {
        "id": "e8PInmVxO2e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "aJEovIe2SApl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path ='/content/drive/MyDrive/datascience/happy_sad_data/train/' \n",
        "test_path ='/content/drive/MyDrive/datascience/happy_sad_data/test/' "
      ],
      "metadata": {
        "id": "P_GfeBZtSHXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "w2LfWGVATF0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sad_happy_fn():\n",
        "  accuracy_requrired = 0.999\n",
        "  class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs={}):\n",
        "      if(logs.get('accuracy')>0.98):\n",
        "        print(\"now loss has been optimized stop training\")\n",
        "        self.model.stop_training  =True\n",
        "    # Calling the Callback\n",
        "  callback = myCallback()\n",
        "  # defining the model\n",
        "  model_emotion = tf.keras.models.Sequential([\n",
        "                        tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',input_shape = (150,150,3)),\n",
        "                        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                        tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n",
        "                        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                        tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n",
        "                        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "                        # Fully COnnected Neural Network\n",
        "                        tf.keras.layers.Flatten(),\n",
        "                        tf.keras.layers.Dense(512,activation='relu'),\n",
        "                        tf.keras.layers.Dense(1,activation='sigmoid')]\n",
        "\n",
        "\n",
        "  )\n",
        "  \n",
        "  model_emotion.compile(optimizer = 'Rmsprop',loss = 'binary_crossentropy',metrics = ['accuracy'])\n",
        "  # creating the training and test data sets by proper labelling\n",
        "  train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "  test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "  train_sad_happy = train_data_gen.flow_from_directory(\n",
        "      train_path,\n",
        "      target_size =(150,150),\n",
        "      batch_size=20,\n",
        "      class_mode = 'binary' )\n",
        "  test_sad_happy = test_data_gen.flow_from_directory(\n",
        "      test_path,\n",
        "      target_size =(150,150),\n",
        "      batch_size=20,\n",
        "      class_mode = 'binary')\n",
        "  history = model_emotion.fit_generator(\n",
        "    train_sad_happy, \n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    validation_data = test_sad_happy,\n",
        "    callbacks = [callback])\n",
        "  #return history.history['acc'][-1]       \n",
        "\n"
      ],
      "metadata": {
        "id": "3KxU4MC8TTnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sad_happy_fn()"
      ],
      "metadata": {
        "id": "BT_-af8yYTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Emotional classifier"
      ],
      "metadata": {
        "id": "KtH4V4LdZEYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "rRN4hp1QZKkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path ='/content/drive/MyDrive/datascience/happy_sad_data'"
      ],
      "metadata": {
        "id": "vwmrCeTIZY6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Content of base directory\")\n",
        "print(os.listdir(path))"
      ],
      "metadata": {
        "id": "izvjhLq8Zl3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# acessing train data\n",
        "print('\\n Content of train directory of happy sad data')\n",
        "print(os.listdir(f'{path}/train'))"
      ],
      "metadata": {
        "id": "u6LSLd5UaM7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nContents of test directory of happy sad data\")\n",
        "print(os.listdir(f'{path}/test'))"
      ],
      "metadata": {
        "id": "mIyFOAFWard6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(path,'train')\n",
        "test_dir = os.path.join(path,'test')"
      ],
      "metadata": {
        "id": "urCVBSoja9AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir_happy = os.path.join(train_dir,'happy')\n",
        "train_dir_sad = os.path.join(train_dir,'Sad')"
      ],
      "metadata": {
        "id": "S4OgE8JjbSGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir_happy  = os.path.join(test_dir,'happy_test')\n",
        "test_dir_sad = os.path.join(test_dir,'sad_test')"
      ],
      "metadata": {
        "id": "ESBe0ejEdt0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REFERENCE TO ACESS THE HAPPY IMAGES\n",
        "train_dir_happy_img = os.listdir(train_dir_happy)"
      ],
      "metadata": {
        "id": "Fu8kaCi6b2w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REFERENCE TO ACESS THE SAD IMAGES\n",
        "train_dir_sad_img = os.listdir(train_dir_sad)"
      ],
      "metadata": {
        "id": "c3sNrAAVcKUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REFERENCE TO ACESS THE TEST HAPPY IMAGES\n",
        "test_dir_happy_img = os.listdir(test_dir_happy)"
      ],
      "metadata": {
        "id": "5L-5vV62dW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REFERENCE TO ACESS THE TEST SAD IMAGES\n",
        "test_dir_sad_img = os.listdir(test_dir_sad)"
      ],
      "metadata": {
        "id": "RfNoBJ63eCI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file name of training images of happy\n",
        "print(train_dir_happy_img[:10])"
      ],
      "metadata": {
        "id": "-Evf499WcWTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file name of training images of sad\n",
        "print(train_dir_sad_img[:10])"
      ],
      "metadata": {
        "id": "fXG7P-0vcizC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file name of test images of happy\n",
        "print(test_dir_happy_img[:10])"
      ],
      "metadata": {
        "id": "5Fcqwud2ernj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file name of training images of sad\n",
        "print(test_dir_sad_img)"
      ],
      "metadata": {
        "id": "xjNCxYwDe0kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets find total number of images happy and sad\n",
        "print('total training cat images :', len(os.listdir(train_dir_happy)))\n",
        "print('total training dog images :', len(os.listdir(train_dir_sad)))\n",
        "\n",
        "print('total TEST cat images :', len(os.listdir(test_dir_happy) ))\n",
        "print('total TEST dog images :', len(os.listdir(test_dir_sad) ))"
      ],
      "metadata": {
        "id": "76TaPXNkf0e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now take a look at a few pictures to get a better sense of what the happy and sad datasets look like. First, configure the matplotlib parameters:"
      ],
      "metadata": {
        "id": "KBdsKGi4hALk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "pic_index = 0 # Index for iterating over images"
      ],
      "metadata": {
        "id": "Rm5o1KE8g5N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols*4, nrows*4)\n",
        "\n",
        "pic_index+=8\n",
        "\n",
        "next_happy_pix = [os.path.join(train_dir_happy, fname) \n",
        "                for fname in train_dir_happy_img[ pic_index-8:pic_index] \n",
        "               ]\n",
        "\n",
        "next_sad_pix = [os.path.join(train_dir_sad, fname) \n",
        "                for fname in train_dir_sad_img[ pic_index-8:pic_index]\n",
        "               ]\n",
        "for i, img_path in enumerate(next_happy_pix+next_sad_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fVb1ULLVhK4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building a model 72 % accuracy\n",
        "import tensorflow as tf\n",
        "\n",
        "model_emotion = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(), \n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'), \n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  \n",
        "]) "
      ],
      "metadata": {
        "id": "z4IbeYHwiS0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model_emotion.compile(optimizer=RMSprop(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "Rfc8bf40iY3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )"
      ],
      "metadata": {
        "id": "reNpZ236kS2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150)) "
      ],
      "metadata": {
        "id": "ZbK2Wpg4kc9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.filenames\n"
      ],
      "metadata": {
        "id": "tFiyZlaAA7Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))"
      ],
      "metadata": {
        "id": "yvzfyI699CJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training of model\n",
        "history = model_emotion.fit(\n",
        "            train_generator,\n",
        "            #steps_per_epoch=100,\n",
        "            epochs=15,\n",
        "            validation_data=test_generator,\n",
        "            #validation_steps=50,\n",
        "            verbose=2\n",
        "            )"
      ],
      "metadata": {
        "id": "CoPoXtfl9KbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=image.load_img(path, target_size=(150, 150))\n",
        "  \n",
        "  x=image.img_to_array(img)\n",
        "  x /= 255.0\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model_emotion.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  \n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" happy\")\n",
        "  else:\n",
        "    print(fn + \" sad\")\n",
        " "
      ],
      "metadata": {
        "id": "QvjOxjWC9SzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visulaizing Images of ConvNets"
      ],
      "metadata": {
        "id": "AAj30C2DBSrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing Intermediate Representations\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
      ],
      "metadata": {
        "id": "Im7zlTWsBXMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "successive_outputs = [layer.output for layer in model_emotion.layers[1:]]\n",
        "visualization_model = tf.keras.models.Model(inputs = model_emotion.input, outputs = successive_outputs)"
      ],
      "metadata": {
        "id": "xvqW5V-zBl3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare a random input image from the training set.\n",
        "# we take some images from happy and some from sad folder\n",
        "happy_images = [os.path.join(train_dir_happy,f) for f in train_dir_happy_img ]\n",
        "sad_images = [os.path.join(train_dir_sad,f) for f in train_dir_sad_img ]"
      ],
      "metadata": {
        "id": "nyy2KauvBqNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = random.choice(happy_images + sad_images)"
      ],
      "metadata": {
        "id": "OYf7gID_D-UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
        "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Scale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Run the image through the network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)"
      ],
      "metadata": {
        "id": "XRHvT-dYEGhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the names of the layers, so you can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model_emotion.layers]\n"
      ],
      "metadata": {
        "id": "aRd0Y2a2ELXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  \n",
        "  if len(feature_map.shape) == 4:\n",
        "    \n",
        "    #-------------------------------------------\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    #-------------------------------------------\n",
        "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
        "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
        "    \n",
        "    # Tile the images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    \n",
        "    #-------------------------------------------------\n",
        "    # Postprocess the feature to be visually palatable\n",
        "    #-------------------------------------------------\n",
        "    for i in range(n_features):\n",
        "      x  = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std ()\n",
        "      x *=  64\n",
        "      x += 128\n",
        "      x  = np.clip(x, 0, 255).astype('uint8')\n",
        "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
        "\n",
        "    #-----------------\n",
        "    # Display the grid\n",
        "    #-----------------\n",
        "    scale = 20. / n_features\n",
        "    plt.figure( figsize=(scale * n_features, scale) )\n",
        "    plt.title ( layer_name )\n",
        "    plt.grid  ( False )\n",
        "    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
      ],
      "metadata": {
        "id": "a5CKiwsgEOuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating Accuracy and Loss for the Model\n",
        "#You will plot the training/validation accuracy and loss as collected during training:\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "metadata": {
        "id": "6LjjtK4YEltw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Augmentaion on happy-sad data for overfitting"
      ],
      "metadata": {
        "id": "jTTawJujvROD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train_datagen = ImageDataGenerator(\n",
        "    rotation_range= 45,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip= True,\n",
        "    fill_mode = 'nearest' \n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "gyhz9OHMveAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_test_datagen = ImageDataGenerator(rescale= 1./255)"
      ],
      "metadata": {
        "id": "OfqxbTRSv3M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train_data_generator = aug_train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")\n"
      ],
      "metadata": {
        "id": "jQOOgsnSv8UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_test_data_generator = aug_test_datagen.flow_from_directory(\n",
        "    test_dir ,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "YeCRqUNSwMLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_emotion.fit(\n",
        "            aug_train_data_generator,\n",
        "            #steps_per_epoch=100,\n",
        "            epochs=15,\n",
        "            validation_data=aug_test_data_generator,\n",
        "            #validation_steps=50,\n",
        "            verbose=2\n",
        "            )"
      ],
      "metadata": {
        "id": "WxKNRJB8worI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating Accuracy and Loss for the Model\n",
        "#You will plot the training/validation accuracy and loss as collected during training:\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "metadata": {
        "id": "COzCkbVo0Ilo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "uzuMkItfZ5Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model,layers\n",
        "# importing the inception model\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "trained_weight = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "metadata": {
        "id": "Atk1R9oaZ8gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making a refernce to Inception model\n",
        "pre_trained_model = InceptionV3(input_shape = (150,150,3),\n",
        "                                include_top = False,\n",
        "                                weights = None)"
      ],
      "metadata": {
        "id": "eDpbBq7Rbl8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the weights\n",
        "pre_trained_model.load_weights(trained_weight)"
      ],
      "metadata": {
        "id": "DnBuDh2tccnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now i will freeze my trainable parameters by writing this code\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "IcSOlALidWxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model.summary()"
      ],
      "metadata": {
        "id": "AUewpeEPd9ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding the model with tranfer weights from inception model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "last_layer = pre_trained_model.get_layer('mixed8')\n",
        "layer_output = last_layer.output\n",
        "#stacking layers in Model using Functional API\n",
        "x = layers.Flatten()(layer_output)\n",
        "x = layers.Dense(1024,activation='relu')(x)\n",
        "x = layers.Dense(1,activation='sigmoid')(x)"
      ],
      "metadata": {
        "id": "j6kmykC0eEiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model \n",
        "model_trans = Model(inputs=pre_trained_model.input,outputs=x)"
      ],
      "metadata": {
        "id": "A3OgnrGLISQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "model_trans.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "kVOsaERIIxFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now for data to ve passed in to the model we use image generator\n",
        "aug_train_datagen = ImageDataGenerator(\n",
        "    rotation_range= 45,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip= True,\n",
        "    fill_mode = 'nearest' \n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "Fb64tOXWKJ5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_test_datagen = ImageDataGenerator(rescale= 1./255)"
      ],
      "metadata": {
        "id": "lskvYFApkoA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train_data_generator = aug_train_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "aygBUhsCk12s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_test_data_generator = aug_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size = (150,150),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "_TNNteMeky2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self,epoch,logs={}):\n",
        "      if(logs.get('accuracy')>0.98):\n",
        "        print(\"now loss has been optimized stop training\")\n",
        "        self.model.stop_training  =True\n",
        "    # Calling the Callback\n"
      ],
      "metadata": {
        "id": "zlwn1anlmOuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = myCallback()\n",
        "history_emotion_tranfer = model_trans.fit(\n",
        "    aug_train_data_generator,\n",
        "    validation_data = aug_test_data_generator,\n",
        "    #steps_per_epoch = 10,\n",
        "    epochs = 100,\n",
        "    #validation_steps=10,\n",
        "    verbose = 2,\n",
        "    callbacks = [callback]\n",
        ")"
      ],
      "metadata": {
        "id": "1t0FGVGclft9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#exploring dropout as a method to avoid overfitting"
      ],
      "metadata": {
        "id": "UrtVCb0NmZl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building a  dropout model\n",
        "#stacking layers in Model using Functional API\n",
        "x_drop = layers.Flatten()(layer_output)\n",
        "x_drop = layers.Dense(1024,activation='relu')(x_drop)\n",
        "# adding a dropout layer\n",
        "x_drop = layers.Dropout(0.2)(x_drop)\n",
        "x_drop = layers.Dense(512,activation='relu')(x_drop)\n",
        "x_drop = layers.Dense(1,activation='sigmoid')(x_drop)"
      ],
      "metadata": {
        "id": "hEgl5O6qmrrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dropout model\n",
        "\n",
        "model_drop = Model(inputs=pre_trained_model.input,outputs=x_drop)"
      ],
      "metadata": {
        "id": "t3TVMDBUnz71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model containing dropout layer\n",
        "model_drop.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "cL_zWQ_hoM2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_drop = model_drop.fit(\n",
        "    aug_train_data_generator,\n",
        "    validation_data = aug_test_data_generator,\n",
        "    #steps_per_epoch = 10,\n",
        "    epochs = 100,\n",
        "    #validation_steps=10,\n",
        "    verbose = 2,\n",
        "    callbacks = [callback]\n",
        ")"
      ],
      "metadata": {
        "id": "6noZ8GQwoXIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Layers and custom loss functions\n",
        "\n",
        "\n",
        "1.   Functional API\n",
        "2.   Custom Loss Function\n",
        "3.   Custom models\n",
        "4.   Custom layers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "csUvLinE9_hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API\n",
        "# The FUnctional API is a grpah of layers as we have more accesibility of acessing layers\n"
      ],
      "metadata": {
        "id": "2XrV1jC7YmCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}